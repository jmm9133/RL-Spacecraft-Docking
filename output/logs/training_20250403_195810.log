2025-04-03 19:58:10,857 [__main__] [INFO] Initializing Ray...
2025-04-03 19:58:16,443 [__main__] [INFO] Creating temporary environment to get action/observation spaces...
2025-04-03 19:58:16,445 [src.satellite_marl_env] [INFO] SatelliteMARLEnv closed.
2025-04-03 19:58:16,445 [__main__] [INFO] Spaces retrieved.
2025-04-03 19:58:16,445 [__main__] [INFO] Configuring RLlib PPO Algorithm...
2025-04-03 19:58:16,446 [__main__] [INFO] Building Algorithm...
2025-04-03 19:58:30,156 [__main__] [INFO] Algorithm Built. Using Policy/Module Class: DefaultPPOTorchRLModule
2025-04-03 19:58:30,156 [__main__] [INFO] 
--- Starting Training for 100 iterations ---
2025-04-03 19:58:32,364 [__main__] [INFO] Iter: 1/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.21s
2025-04-03 19:58:34,580 [__main__] [INFO] Iter: 2/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.22s
2025-04-03 19:58:36,856 [__main__] [INFO] Iter: 3/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.28s
2025-04-03 19:58:38,910 [__main__] [INFO] Iter: 4/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.05s
2025-04-03 19:58:41,375 [__main__] [INFO] Iter: 5/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.46s
2025-04-03 19:58:43,738 [__main__] [INFO] Iter: 6/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.36s
2025-04-03 19:58:45,885 [__main__] [INFO] Iter: 7/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.15s
2025-04-03 19:58:48,562 [__main__] [INFO] Iter: 8/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.68s
2025-04-03 19:58:51,183 [__main__] [INFO] Iter: 9/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.62s
2025-04-03 19:58:54,957 [__main__] [INFO] Iter: 10/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 3.77s
2025-04-03 19:58:57,143 [__main__] [INFO] Iter: 11/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.19s
2025-04-03 19:58:59,442 [__main__] [INFO] Iter: 12/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.30s
2025-04-03 19:59:01,883 [__main__] [INFO] Iter: 13/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.44s
2025-04-03 19:59:04,307 [__main__] [INFO] Iter: 14/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.42s
2025-04-03 19:59:06,826 [__main__] [INFO] Iter: 15/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.52s
2025-04-03 19:59:08,977 [__main__] [INFO] Iter: 16/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.15s
2025-04-03 19:59:11,156 [__main__] [INFO] Iter: 17/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.18s
2025-04-03 19:59:13,908 [__main__] [INFO] Iter: 18/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.75s
2025-04-03 19:59:16,234 [__main__] [INFO] Iter: 19/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.32s
2025-04-03 19:59:19,880 [__main__] [INFO] Iter: 20/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 3.65s
2025-04-03 19:59:19,900 [__main__] [INFO] Checkpoint saved in directory TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/Users/nathanleo/Downloads/IST 597 - RL/Final Project/RL-Spacecraft-Docking/output/ray_results), metrics={'timers': {'training_iteration': 2.24158004501885, 'restore_env_runners': 1.53088543175251e-05, 'training_step': 2.2414539982298463, 'env_runner_sampling_timer': 0.25079501760399503, 'learner_update_timer': 1.9845354753629842, 'synch_weights': 0.004837187686242826, 'synch_env_connectors': 0.007416995370810094, 'restore_eval_env_runners': 5.378747300710529e-06, 'evaluation_iteration': 3.114647233839496, 'synch_eval_env_connectors': 0.0030808079084090425}, 'env_runners': {'num_agent_steps_sampled': {'target': 4000, 'servicer': 4000}, 'env_to_module_connector': {'timers': {'connectors': {'numpy_to_tensor': 3.072390645464621e-05, 'add_observations_from_episodes_to_batch': 1.6448031172262453e-05, 'batch_individual_items': 1.8334742500894875e-05, 'add_time_dim_to_batch_and_zero_pad': 6.549818784067663e-06, 'agent_to_module_mapping': 4.417519619576092e-06, 'add_states_from_episodes_to_batch': 4.08461180456344e-06}}, 'connector_pipeline_timer': 0.00011674265939099789}, 'num_agent_steps_sampled_lifetime': {'servicer': 80000, 'target': 80000}, 'num_module_steps_sampled_lifetime': {'target': 80000, 'servicer': 80000}, 'module_to_env_connector': {'timers': {'connectors': {'listify_data_for_vector_env': 1.0008292252654102e-05, 'get_actions': 0.00013617812325044076, 'un_batch_to_individual_items': 2.1748314210447143e-05, 'module_to_agent_unmapping': 3.72810497037063e-06, 'tensor_to_numpy': 4.97852699028082e-05, 'remove_single_ts_time_rank_from_batch': 1.212567265070433e-06, 'normalize_and_clip_actions': 5.4949033147933697e-05}}, 'connector_pipeline_timer': 0.0003239338305335091}, 'num_module_steps_sampled': {'target': 4000, 'servicer': 4000}, 'timers': {'connectors': {'agent_to_module_mapping': 7.364130199319966e-06, 'add_states_from_episodes_to_batch': 8.177606935572365e-06, 'numpy_to_tensor': 6.412865237191157e-05, 'add_observations_from_episodes_to_batch': 4.1266348410357274e-05, 'batch_individual_items': 3.793108866691751e-05, 'add_time_dim_to_batch_and_zero_pad': 2.1670348992895175e-05}}, 'num_env_steps_sampled_lifetime': 80000, 'num_env_steps_sampled': 4000, 'connector_pipeline_timer': 0.0003306576083717949, 'env_reset_timer': 0.0001808261307735887, 'env_to_module_sum_episodes_length_out': 103.42885547936852, 'sample': 0.14460899064037833, 'env_step_timer': 0.00010518062648267726, 'env_to_module_sum_episodes_length_in': 103.42885547936852, 'rlmodule_inference_timer': 0.00015311259068094896, 'weights_seq_no': 19.0, 'time_between_sampling': 2.094561346128498, 'episode_len_min': 1000, 'module_episode_returns_mean': {'target': 0.0, 'servicer': -1047.3293539886758}, 'agent_episode_returns_mean': {'target': 0.0, 'servicer': -1047.3293539886758}, 'agent_steps': {'servicer': 1000.0, 'target': 1000.0}, 'episode_return_min': -1055.2941294552513, 'episode_duration_sec_mean': 0.8826718494326442, 'episode_len_mean': 1000.0, 'num_episodes_lifetime': 69, 'episode_return_max': -1036.412251263903, 'episode_len_max': 1000, 'num_episodes': 0, 'episode_return_mean': -1047.3293539886758, 'num_env_steps_sampled_lifetime_throughput': 1097.0202919679423}, 'learners': {'servicer': {'vf_loss_unclipped': 3441.82373046875, 'num_module_steps_trained': 40320, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'curr_entropy_coeff': 0.01, 'policy_loss': 0.052662938833236694, 'curr_kl_coeff': 1.5258789289873675e-06, 'gradients_default_optimizer_global_norm': 0.9896837472915649, 'total_loss': 9.661012649536133, 'vf_loss': 9.694499969482422, 'vf_explained_var': -5.7578086853027344e-05, 'mean_kl_loss': 0.004566561430692673, 'module_train_batch_size_mean': 128.0, 'entropy': 8.61496353149414, 'num_module_steps_trained_lifetime': 807168, 'num_trainable_parameters': 142093.0, 'default_optimizer_learning_rate': 5e-05, 'weights_seq_no': 20.0}, 'target': {'default_optimizer_learning_rate': 5e-05, 'weights_seq_no': 20.0, 'num_module_steps_trained': 40320, 'vf_loss_unclipped': 8.206201300708926e-07, 'curr_entropy_coeff': 0.01, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'policy_loss': -0.06744842231273651, 'curr_kl_coeff': 0.0015625000232830644, 'gradients_default_optimizer_global_norm': 0.5241696238517761, 'total_loss': -0.16000352799892426, 'vf_loss': 8.206201300708926e-07, 'mean_kl_loss': 0.0037854257971048355, 'vf_explained_var': 0.9984877705574036, 'module_train_batch_size_mean': 128.0, 'entropy': 9.2567777633667, 'num_module_steps_trained_lifetime': 807168, 'num_trainable_parameters': 142093.0}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'general_advantage_estimation': 0.025998521297406277, 'add_states_from_episodes_to_batch': 4.938407241054016e-06, 'numpy_to_tensor': 0.00015297068826593713, 'add_observations_from_episodes_to_batch': 0.00021558930510291242, 'add_one_ts_to_episodes_and_truncate': 0.004964081885963093, 'batch_individual_items': 0.060655103363690115, 'add_time_dim_to_batch_and_zero_pad': 1.9482331824255025e-05, 'add_columns_from_episodes_to_train_batch': 0.0674740362783524, 'agent_to_module_mapping': 0.0033703891976943584}}, 'connector_pipeline_timer': 0.16304492195314002}, 'num_module_steps_trained': 80640, 'learner_connector_sum_episodes_length_out': 4000.0, 'num_non_trainable_parameters': 0.0, 'num_env_steps_trained': 1260000, 'learner_connector_sum_episodes_length_in': 4000.0, 'num_env_steps_trained_lifetime': 25224000, 'num_module_steps_trained_lifetime': 1614336, 'num_trainable_parameters': 284186.0, 'num_env_steps_trained_lifetime_throughput': 0.0}}, 'num_training_step_calls_per_iteration': 1, 'evaluation': {'env_runners': {'num_agent_steps_sampled': {'target': 5000, 'servicer': 5000}, 'env_to_module_connector': {'timers': {'connectors': {'numpy_to_tensor': 2.0556467976526116e-05, 'add_observations_from_episodes_to_batch': 1.1762097228577723e-05, 'batch_individual_items': 1.2654878128562044e-05, 'add_time_dim_to_batch_and_zero_pad': 4.565998112263877e-06, 'agent_to_module_mapping': 3.0045632933702052e-06, 'add_states_from_episodes_to_batch': 3.005577642704669e-06}}, 'connector_pipeline_timer': 8.270178245563989e-05}, 'num_agent_steps_sampled_lifetime': {'servicer': 10000, 'target': 10000}, 'num_module_steps_sampled_lifetime': {'target': 10000, 'servicer': 10000}, 'module_to_env_connector': {'timers': {'connectors': {'listify_data_for_vector_env': 7.893352958801277e-06, 'get_actions': 4.050713857992281e-05, 'un_batch_to_individual_items': 1.5555203831413922e-05, 'module_to_agent_unmapping': 2.6231166274608806e-06, 'tensor_to_numpy': 3.793115061372119e-05, 'remove_single_ts_time_rank_from_batch': 8.545523202046402e-07, 'normalize_and_clip_actions': 4.2723920037363674e-05}}, 'connector_pipeline_timer': 0.00018178462886950778}, 'episode_len_min': 1000, 'module_episode_returns_mean': {'target': 0.0, 'servicer': -1029.9960140068272}, 'num_module_steps_sampled': {'target': 5000, 'servicer': 5000}, 'timers': {'connectors': {'agent_to_module_mapping': 6.000610330374912e-06, 'add_states_from_episodes_to_batch': 5.082990240771323e-06, 'numpy_to_tensor': 3.375748255493818e-05, 'add_observations_from_episodes_to_batch': 3.2708209146221635e-05, 'batch_individual_items': 2.699964766361518e-05, 'add_time_dim_to_batch_and_zero_pad': 2.041625225247117e-05}}, 'num_env_steps_sampled_lifetime': 10000, 'agent_episode_returns_mean': {'target': 0.0, 'servicer': -1029.9960140068272}, 'agent_steps': {'servicer': 1000.0, 'target': 1000.0}, 'episode_return_min': -1033.5554685966445, 'episode_duration_sec_mean': 0.5623233193357009, 'episode_len_mean': 1000.0, 'num_episodes_lifetime': 10, 'episode_return_max': -1022.8771048271914, 'num_env_steps_sampled': 5000, 'connector_pipeline_timer': 0.00024029217381903436, 'env_reset_timer': 0.000163537333982822, 'episode_len_max': 1000, 'env_to_module_sum_episodes_length_out': 901.0423566894266, 'num_episodes': 5, 'sample': 2.9325906114997573, 'env_step_timer': 8.520085740590567e-05, 'env_to_module_sum_episodes_length_in': 901.0423566894266, 'episode_return_mean': -1029.9960140068272, 'rlmodule_inference_timer': 0.00012079680984628102, 'weights_seq_no': 19.0, 'num_env_steps_sampled_per_second': 1604.8594074666416, 'time_between_sampling': 22.100166832999093}, 'num_healthy_workers': 1, 'actor_manager_num_outstanding_async_reqs': 0, 'num_remote_worker_restarts': 0}, 'num_env_steps_sampled_lifetime': 80000, 'fault_tolerance': {'num_healthy_workers': 23, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0}, 'num_env_steps_sampled_lifetime_throughput': 1097.0202919679423, 'done': False, 'training_iteration': 20, 'trial_id': 'default', 'date': '2025-04-03_19-59-19', 'timestamp': 1743724759, 'time_this_iter_s': 3.633301019668579, 'time_total_s': 49.481619358062744, 'pid': 72496, 'hostname': 'Nathans-Mac-Studio.local', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'satellite_marl', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 23, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 10, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function <lambda> at 0x36f5314e0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': True, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.01, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'servicer': (None, Box(-inf, inf, (13,), float32), Box(-1.0, 1.0, (6,), float32), None), 'target': (None, Box(-inf, inf, (13,), float32), Box(-1.0, 1.0, (6,), float32), None)}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 49.481619358062744, 'iterations_since_restore': 20, 'perf': {'cpu_util_percent': 80.1, 'ram_util_percent': 56.1}})
2025-04-03 19:59:22,320 [__main__] [INFO] Iter: 21/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.42s
2025-04-03 19:59:24,547 [__main__] [INFO] Iter: 22/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.23s
2025-04-03 19:59:26,862 [__main__] [INFO] Iter: 23/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.32s
2025-04-03 19:59:29,413 [__main__] [INFO] Iter: 24/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.55s
2025-04-03 19:59:31,989 [__main__] [INFO] Iter: 25/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.58s
2025-04-03 19:59:34,504 [__main__] [INFO] Iter: 26/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.51s
2025-04-03 19:59:36,917 [__main__] [INFO] Iter: 27/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.41s
2025-04-03 19:59:39,536 [__main__] [INFO] Iter: 28/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.62s
2025-04-03 19:59:41,865 [__main__] [INFO] Iter: 29/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.33s
2025-04-03 19:59:45,281 [__main__] [INFO] Iter: 30/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 3.42s
2025-04-03 19:59:47,814 [__main__] [INFO] Iter: 31/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.53s
2025-04-03 19:59:50,461 [__main__] [INFO] Iter: 32/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.65s
2025-04-03 19:59:52,504 [__main__] [INFO] Iter: 33/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.04s
2025-04-03 19:59:55,052 [__main__] [INFO] Iter: 34/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.55s
2025-04-03 19:59:57,596 [__main__] [INFO] Iter: 35/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.54s
2025-04-03 20:00:00,142 [__main__] [INFO] Iter: 36/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.55s
2025-04-03 20:00:02,490 [__main__] [INFO] Iter: 37/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.35s
2025-04-03 20:00:04,886 [__main__] [INFO] Iter: 38/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.40s
2025-04-03 20:00:07,291 [__main__] [INFO] Iter: 39/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.40s
2025-04-03 20:00:10,791 [__main__] [INFO] Iter: 40/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 3.50s
2025-04-03 20:00:10,809 [__main__] [INFO] Checkpoint saved in directory TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/Users/nathanleo/Downloads/IST 597 - RL/Final Project/RL-Spacecraft-Docking/output/ray_results), metrics={'timers': {'training_iteration': 2.2928081472807658, 'restore_env_runners': 1.432380142097739e-05, 'training_step': 2.2926781735036497, 'env_runner_sampling_timer': 0.24377896327275295, 'learner_update_timer': 2.0426651670029177, 'synch_weights': 0.004836041722253623, 'synch_env_connectors': 0.007392976464065948, 'restore_eval_env_runners': 5.393050216225674e-06, 'evaluation_iteration': 3.1130912047435264, 'synch_eval_env_connectors': 0.0030551638442490223}, 'env_runners': {'num_agent_steps_sampled': {'target': 4000, 'servicer': 4000}, 'env_to_module_connector': {'timers': {'connectors': {'numpy_to_tensor': 3.412618830993201e-05, 'add_observations_from_episodes_to_batch': 1.7410217239286185e-05, 'batch_individual_items': 1.925942086337901e-05, 'add_time_dim_to_batch_and_zero_pad': 7.767984979291278e-06, 'agent_to_module_mapping': 4.576855014098719e-06, 'add_states_from_episodes_to_batch': 4.553132068973288e-06}}, 'connector_pipeline_timer': 0.00012646493668223388}, 'num_agent_steps_sampled_lifetime': {'servicer': 160000, 'target': 160000}, 'num_module_steps_sampled_lifetime': {'target': 160000, 'servicer': 160000}, 'module_to_env_connector': {'timers': {'connectors': {'listify_data_for_vector_env': 1.0800147406474122e-05, 'get_actions': 0.00014603700052670654, 'un_batch_to_individual_items': 2.7121639716118514e-05, 'module_to_agent_unmapping': 3.984330911754946e-06, 'tensor_to_numpy': 5.137805822365958e-05, 'remove_single_ts_time_rank_from_batch': 1.1916718659635327e-06, 'normalize_and_clip_actions': 6.122253219270351e-05}}, 'connector_pipeline_timer': 0.00035104245637333076}, 'num_module_steps_sampled': {'target': 4000, 'servicer': 4000}, 'timers': {'connectors': {'agent_to_module_mapping': 7.364130199319966e-06, 'add_states_from_episodes_to_batch': 8.177606935572365e-06, 'numpy_to_tensor': 6.412865237191157e-05, 'add_observations_from_episodes_to_batch': 4.1266348410357274e-05, 'batch_individual_items': 3.793108866691751e-05, 'add_time_dim_to_batch_and_zero_pad': 2.1670348992895175e-05}}, 'num_env_steps_sampled_lifetime': 160000, 'num_env_steps_sampled': 4000, 'connector_pipeline_timer': 0.0003306576083717949, 'env_reset_timer': 0.0001808261307735887, 'env_to_module_sum_episodes_length_out': 106.27241732416265, 'sample': 0.14679873283857198, 'env_step_timer': 0.00011183651784655823, 'env_to_module_sum_episodes_length_in': 106.27241732416265, 'rlmodule_inference_timer': 0.00016999117246022086, 'weights_seq_no': 39.0, 'time_between_sampling': 2.1480902196122034, 'episode_len_min': 1000, 'module_episode_returns_mean': {'target': 0.0, 'servicer': -1035.9408415159476}, 'agent_episode_returns_mean': {'target': 0.0, 'servicer': -1035.9408415159476}, 'agent_steps': {'servicer': 1000.0, 'target': 1000.0}, 'episode_return_min': -1046.4873276393469, 'episode_duration_sec_mean': 0.8934173527744387, 'episode_len_mean': 1000.0, 'num_episodes_lifetime': 138, 'episode_return_max': -1026.995550564325, 'episode_len_max': 1000, 'num_episodes': 0, 'episode_return_mean': -1035.9408415159476, 'num_env_steps_sampled_lifetime_throughput': 1143.3471487775532}, 'learners': {'servicer': {'vf_loss_unclipped': 3245.521728515625, 'num_module_steps_trained': 40320, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'curr_entropy_coeff': 0.01, 'policy_loss': 0.0956558883190155, 'curr_kl_coeff': 2.9802322831784522e-09, 'gradients_default_optimizer_global_norm': 1.684786319732666, 'total_loss': 9.658758163452148, 'vf_loss': 9.651747703552246, 'vf_explained_var': -0.00029158592224121094, 'mean_kl_loss': 0.006043858360499144, 'module_train_batch_size_mean': 128.0, 'entropy': 8.864734649658203, 'num_module_steps_trained_lifetime': 1614208, 'num_trainable_parameters': 142093.0, 'default_optimizer_learning_rate': 5e-05, 'weights_seq_no': 40.0}, 'target': {'default_optimizer_learning_rate': 5e-05, 'weights_seq_no': 40.0, 'num_module_steps_trained': 40320, 'vf_loss_unclipped': 3.3992105841207376e-07, 'curr_entropy_coeff': 0.01, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'policy_loss': -0.0017108968459069729, 'curr_kl_coeff': 0.0003906250058207661, 'gradients_default_optimizer_global_norm': 0.37576523423194885, 'total_loss': -0.12324533611536026, 'vf_loss': 3.3992105841207376e-07, 'mean_kl_loss': 0.01133115217089653, 'vf_explained_var': 0.9990372657775879, 'module_train_batch_size_mean': 128.0, 'entropy': 12.15392017364502, 'num_module_steps_trained_lifetime': 1614208, 'num_trainable_parameters': 142093.0}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'general_advantage_estimation': 0.025470693890112243, 'add_states_from_episodes_to_batch': 5.038917999250336e-06, 'numpy_to_tensor': 0.00015233074603138914, 'add_observations_from_episodes_to_batch': 0.00021772873573548096, 'add_one_ts_to_episodes_and_truncate': 0.00497361629969908, 'batch_individual_items': 0.06159063978066287, 'add_time_dim_to_batch_and_zero_pad': 1.9350957114081573e-05, 'add_columns_from_episodes_to_train_batch': 0.06766179819172222, 'agent_to_module_mapping': 0.003404222695142607}}, 'connector_pipeline_timer': 0.16368332713192912}, 'num_module_steps_trained': 80640, 'learner_connector_sum_episodes_length_out': 4000.0, 'num_non_trainable_parameters': 0.0, 'num_env_steps_trained': 1260000, 'learner_connector_sum_episodes_length_in': 4000.0, 'num_env_steps_trained_lifetime': 50444000, 'num_module_steps_trained_lifetime': 3228416, 'num_trainable_parameters': 284186.0, 'num_env_steps_trained_lifetime_throughput': 0.0}}, 'num_training_step_calls_per_iteration': 1, 'evaluation': {'env_runners': {'num_agent_steps_sampled': {'target': 5000, 'servicer': 5000}, 'env_to_module_connector': {'timers': {'connectors': {'numpy_to_tensor': 2.0717715760166524e-05, 'add_observations_from_episodes_to_batch': 1.1813647303425557e-05, 'batch_individual_items': 1.2783219879590034e-05, 'add_time_dim_to_batch_and_zero_pad': 4.580901989457111e-06, 'agent_to_module_mapping': 3.043491298103096e-06, 'add_states_from_episodes_to_batch': 3.0099495921830316e-06}}, 'connector_pipeline_timer': 8.319965621694443e-05}, 'num_agent_steps_sampled_lifetime': {'servicer': 20000, 'target': 20000}, 'num_module_steps_sampled_lifetime': {'target': 20000, 'servicer': 20000}, 'module_to_env_connector': {'timers': {'connectors': {'listify_data_for_vector_env': 7.892007577612244e-06, 'get_actions': 4.06916143430802e-05, 'un_batch_to_individual_items': 1.564500150415299e-05, 'module_to_agent_unmapping': 2.6277288966187153e-06, 'tensor_to_numpy': 3.8112838817081194e-05, 'remove_single_ts_time_rank_from_batch': 8.566386677013405e-07, 'normalize_and_clip_actions': 4.302979687828789e-05}}, 'connector_pipeline_timer': 0.00018306891672836262}, 'episode_len_min': 1000, 'module_episode_returns_mean': {'target': 0.0, 'servicer': -1024.2534173158836}, 'num_module_steps_sampled': {'target': 5000, 'servicer': 5000}, 'timers': {'connectors': {'agent_to_module_mapping': 6.001318794945837e-06, 'add_states_from_episodes_to_batch': 5.082510701597203e-06, 'numpy_to_tensor': 3.377347023513183e-05, 'add_observations_from_episodes_to_batch': 3.270392964214626e-05, 'batch_individual_items': 2.6997291290839352e-05, 'add_time_dim_to_batch_and_zero_pad': 2.041127434555543e-05}}, 'num_env_steps_sampled_lifetime': 20000, 'agent_episode_returns_mean': {'target': 0.0, 'servicer': -1024.2534173158836}, 'agent_steps': {'servicer': 1000.0, 'target': 1000.0}, 'episode_return_min': -1038.7649054259614, 'episode_duration_sec_mean': 0.5561476666020462, 'episode_len_mean': 1000.0, 'num_episodes_lifetime': 20, 'episode_return_max': -1000.4580394323609, 'num_env_steps_sampled': 5000, 'connector_pipeline_timer': 0.00024026255112136394, 'env_reset_timer': 0.0001635146408959907, 'episode_len_max': 1000, 'env_to_module_sum_episodes_length_out': 901.0423566894266, 'num_episodes': 5, 'sample': 2.932540685148299, 'env_step_timer': 8.53373269984341e-05, 'env_to_module_sum_episodes_length_in': 901.0423566894266, 'episode_return_mean': -1024.2534173158836, 'rlmodule_inference_timer': 0.00012108763795258984, 'weights_seq_no': 39.0, 'num_env_steps_sampled_per_second': 1604.8715068793754, 'time_between_sampling': 22.10036293259903}, 'num_healthy_workers': 1, 'actor_manager_num_outstanding_async_reqs': 0, 'num_remote_worker_restarts': 0}, 'num_env_steps_sampled_lifetime': 160000, 'fault_tolerance': {'num_healthy_workers': 23, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0}, 'num_env_steps_sampled_lifetime_throughput': 1143.3471487775532, 'done': False, 'training_iteration': 40, 'trial_id': 'default', 'date': '2025-04-03_20-00-10', 'timestamp': 1743724810, 'time_this_iter_s': 3.4849038124084473, 'time_total_s': 100.05934500694275, 'pid': 72496, 'hostname': 'Nathans-Mac-Studio.local', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'satellite_marl', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 23, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 10, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function <lambda> at 0x36f5314e0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': True, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.01, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'servicer': (None, Box(-inf, inf, (13,), float32), Box(-1.0, 1.0, (6,), float32), None), 'target': (None, Box(-inf, inf, (13,), float32), Box(-1.0, 1.0, (6,), float32), None)}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 100.05934500694275, 'iterations_since_restore': 40, 'perf': {'cpu_util_percent': 82.38000000000001, 'ram_util_percent': 56.120000000000005}})
2025-04-03 20:00:13,314 [__main__] [INFO] Iter: 41/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.50s
2025-04-03 20:00:15,668 [__main__] [INFO] Iter: 42/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.35s
2025-04-03 20:00:17,786 [__main__] [INFO] Iter: 43/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.12s
2025-04-03 20:00:20,322 [__main__] [INFO] Iter: 44/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.54s
2025-04-03 20:00:22,950 [__main__] [INFO] Iter: 45/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.63s
2025-04-03 20:00:25,439 [__main__] [INFO] Iter: 46/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.49s
2025-04-03 20:00:27,971 [__main__] [INFO] Iter: 47/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.53s
2025-04-03 20:00:30,262 [__main__] [INFO] Iter: 48/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.29s
2025-04-03 20:00:32,647 [__main__] [INFO] Iter: 49/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.38s
2025-04-03 20:00:36,002 [__main__] [INFO] Iter: 50/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 3.35s
2025-04-03 20:00:38,062 [__main__] [INFO] Iter: 51/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.06s
2025-04-03 20:00:40,496 [__main__] [INFO] Iter: 52/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.43s
2025-04-03 20:00:42,751 [__main__] [INFO] Iter: 53/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.25s
2025-04-03 20:00:45,216 [__main__] [INFO] Iter: 54/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.47s
2025-04-03 20:00:47,544 [__main__] [INFO] Iter: 55/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.33s
2025-04-03 20:00:49,920 [__main__] [INFO] Iter: 56/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.38s
2025-04-03 20:00:52,426 [__main__] [INFO] Iter: 57/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.51s
2025-04-03 20:00:54,625 [__main__] [INFO] Iter: 58/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.20s
2025-04-03 20:00:56,927 [__main__] [INFO] Iter: 59/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.30s
2025-04-03 20:01:00,169 [__main__] [INFO] Iter: 60/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 3.24s
2025-04-03 20:01:00,190 [__main__] [INFO] Checkpoint saved in directory TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/Users/nathanleo/Downloads/IST 597 - RL/Final Project/RL-Spacecraft-Docking/output/ray_results), metrics={'timers': {'training_iteration': 2.320104763385225, 'restore_env_runners': 1.3327946999875823e-05, 'training_step': 2.3199736079241022, 'env_runner_sampling_timer': 0.23686521682706568, 'learner_update_timer': 2.0767991602319684, 'synch_weights': 0.004830788296955044, 'synch_env_connectors': 0.007403481689842982, 'restore_eval_env_runners': 5.407271107127408e-06, 'evaluation_iteration': 3.109031372531696, 'synch_eval_env_connectors': 0.003022725733829723}, 'env_runners': {'num_agent_steps_sampled': {'target': 4000, 'servicer': 4000}, 'env_to_module_connector': {'timers': {'connectors': {'numpy_to_tensor': 3.1608011032269194e-05, 'add_observations_from_episodes_to_batch': 1.7375853789310504e-05, 'batch_individual_items': 1.850301285015066e-05, 'add_time_dim_to_batch_and_zero_pad': 7.068904098630847e-06, 'agent_to_module_mapping': 4.314777465998083e-06, 'add_states_from_episodes_to_batch': 4.337095457731395e-06}}, 'connector_pipeline_timer': 0.0001212983185172038}, 'num_agent_steps_sampled_lifetime': {'servicer': 240000, 'target': 240000}, 'num_module_steps_sampled_lifetime': {'target': 240000, 'servicer': 240000}, 'module_to_env_connector': {'timers': {'connectors': {'listify_data_for_vector_env': 1.0210633746043608e-05, 'get_actions': 0.00014097714542354644, 'un_batch_to_individual_items': 2.2764537788559498e-05, 'module_to_agent_unmapping': 4.168112766179541e-06, 'tensor_to_numpy': 5.106140512512268e-05, 'remove_single_ts_time_rank_from_batch': 1.4903559510834172e-06, 'normalize_and_clip_actions': 5.731699583812972e-05}}, 'connector_pipeline_timer': 0.00033410656162496754}, 'num_module_steps_sampled': {'target': 4000, 'servicer': 4000}, 'timers': {'connectors': {'agent_to_module_mapping': 7.364130199319966e-06, 'add_states_from_episodes_to_batch': 8.177606935572365e-06, 'numpy_to_tensor': 6.412865237191157e-05, 'add_observations_from_episodes_to_batch': 4.1266348410357274e-05, 'batch_individual_items': 3.793108866691751e-05, 'add_time_dim_to_batch_and_zero_pad': 2.1670348992895175e-05}}, 'num_env_steps_sampled_lifetime': 240000, 'num_env_steps_sampled': 4000, 'connector_pipeline_timer': 0.0003306576083717949, 'env_reset_timer': 0.0001808261307735887, 'env_to_module_sum_episodes_length_out': 101.27250081465326, 'sample': 0.14868115161474876, 'env_step_timer': 0.00010671461886800401, 'env_to_module_sum_episodes_length_in': 101.27250081465326, 'rlmodule_inference_timer': 0.000156501671516227, 'weights_seq_no': 59.0, 'time_between_sampling': 2.1827781523097247, 'episode_len_min': 1000, 'module_episode_returns_mean': {'target': 0.0, 'servicer': -1067.927698526414}, 'agent_episode_returns_mean': {'target': 0.0, 'servicer': -1067.927698526414}, 'agent_steps': {'servicer': 1000.0, 'target': 1000.0}, 'episode_return_min': -1093.7530399959721, 'episode_duration_sec_mean': 0.8522636939546235, 'episode_len_mean': 1000.0, 'num_episodes_lifetime': 230, 'episode_return_max': -1046.925122203675, 'episode_len_max': 1000, 'num_episodes': 0, 'episode_return_mean': -1067.927698526414, 'num_env_steps_sampled_lifetime_throughput': 1234.2559563699076}, 'learners': {'servicer': {'vf_loss_unclipped': 3193.88623046875, 'num_module_steps_trained': 40320, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'curr_entropy_coeff': 0.01, 'policy_loss': -0.052692968398332596, 'curr_kl_coeff': 3.6379788613018216e-13, 'gradients_default_optimizer_global_norm': 1.2836090326309204, 'total_loss': 9.64090347290039, 'vf_loss': 9.791213989257812, 'vf_explained_var': -0.00015091896057128906, 'mean_kl_loss': 0.006700084079056978, 'module_train_batch_size_mean': 128.0, 'entropy': 9.761756896972656, 'num_module_steps_trained_lifetime': 2421376, 'num_trainable_parameters': 142093.0, 'default_optimizer_learning_rate': 5e-05, 'weights_seq_no': 60.0}, 'target': {'default_optimizer_learning_rate': 5e-05, 'weights_seq_no': 60.0, 'num_module_steps_trained': 40320, 'vf_loss_unclipped': 2.9599862045870395e-06, 'curr_entropy_coeff': 0.01, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'policy_loss': 0.004511379636824131, 'curr_kl_coeff': 0.0019775391556322575, 'gradients_default_optimizer_global_norm': 0.5472435355186462, 'total_loss': -0.16560187935829163, 'vf_loss': 2.9599862045870395e-06, 'mean_kl_loss': 0.01806144043803215, 'vf_explained_var': 0.9998988509178162, 'module_train_batch_size_mean': 128.0, 'entropy': 17.015193939208984, 'num_module_steps_trained_lifetime': 2421376, 'num_trainable_parameters': 142093.0}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'general_advantage_estimation': 0.024697680683760385, 'add_states_from_episodes_to_batch': 5.105439889987857e-06, 'numpy_to_tensor': 0.00015113451624681698, 'add_observations_from_episodes_to_batch': 0.0002207124594855632, 'add_one_ts_to_episodes_and_truncate': 0.004985274953285104, 'batch_individual_items': 0.06288672599973265, 'add_time_dim_to_batch_and_zero_pad': 1.92032126194722e-05, 'add_columns_from_episodes_to_train_batch': 0.06794678871681638, 'agent_to_module_mapping': 0.0034358690260526802}}, 'connector_pipeline_timer': 0.16453277267287097}, 'num_module_steps_trained': 80640, 'learner_connector_sum_episodes_length_out': 4000.0, 'num_non_trainable_parameters': 0.0, 'num_env_steps_trained': 1260000, 'learner_connector_sum_episodes_length_in': 4000.0, 'num_env_steps_trained_lifetime': 75668000, 'num_module_steps_trained_lifetime': 4842752, 'num_trainable_parameters': 284186.0, 'num_env_steps_trained_lifetime_throughput': 0.0}}, 'num_training_step_calls_per_iteration': 1, 'evaluation': {'env_runners': {'num_agent_steps_sampled': {'target': 5000, 'servicer': 5000}, 'env_to_module_connector': {'timers': {'connectors': {'numpy_to_tensor': 2.0663096606087616e-05, 'add_observations_from_episodes_to_batch': 1.1786409952166032e-05, 'batch_individual_items': 1.2758676882680967e-05, 'add_time_dim_to_batch_and_zero_pad': 4.568635143973205e-06, 'agent_to_module_mapping': 3.0387544603888364e-06, 'add_states_from_episodes_to_batch': 3.0102764441111243e-06}}, 'connector_pipeline_timer': 8.303268130735237e-05}, 'num_agent_steps_sampled_lifetime': {'servicer': 30000, 'target': 30000}, 'num_module_steps_sampled_lifetime': {'target': 30000, 'servicer': 30000}, 'module_to_env_connector': {'timers': {'connectors': {'listify_data_for_vector_env': 7.870484109633133e-06, 'get_actions': 4.055785320427904e-05, 'un_batch_to_individual_items': 1.561505898169119e-05, 'module_to_agent_unmapping': 2.6228761855754918e-06, 'tensor_to_numpy': 3.800273050702214e-05, 'remove_single_ts_time_rank_from_batch': 8.55156010903911e-07, 'normalize_and_clip_actions': 4.286052883953962e-05}}, 'connector_pipeline_timer': 0.00018252547615876393}, 'episode_len_min': 1000, 'module_episode_returns_mean': {'target': 0.0, 'servicer': -1024.1229144796503}, 'num_module_steps_sampled': {'target': 5000, 'servicer': 5000}, 'timers': {'connectors': {'agent_to_module_mapping': 6.001355123504165e-06, 'add_states_from_episodes_to_batch': 5.081489585971034e-06, 'numpy_to_tensor': 3.3792239885288016e-05, 'add_observations_from_episodes_to_batch': 3.269599959165839e-05, 'batch_individual_items': 2.6992600825462535e-05, 'add_time_dim_to_batch_and_zero_pad': 2.0402037104476662e-05}}, 'num_env_steps_sampled_lifetime': 30000, 'agent_episode_returns_mean': {'target': 0.0, 'servicer': -1024.1229144796503}, 'agent_steps': {'servicer': 1000.0, 'target': 1000.0}, 'episode_return_min': -1039.833957212197, 'episode_duration_sec_mean': 0.5500905699710711, 'episode_len_mean': 1000.0, 'num_episodes_lifetime': 30, 'episode_return_max': -1000.4580394323609, 'num_env_steps_sampled': 5000, 'connector_pipeline_timer': 0.00024019516777456373, 'env_reset_timer': 0.00016346986551964265, 'episode_len_max': 1000, 'env_to_module_sum_episodes_length_out': 901.0423566894266, 'num_episodes': 5, 'sample': 2.932416729437285, 'env_step_timer': 8.509856399584832e-05, 'env_to_module_sum_episodes_length_in': 901.0423566894266, 'episode_return_mean': -1024.1229144796503, 'rlmodule_inference_timer': 0.00012067770176861217, 'weights_seq_no': 59.0, 'num_env_steps_sampled_per_second': 1604.9070521045808, 'time_between_sampling': 22.10062920648662}, 'num_healthy_workers': 1, 'actor_manager_num_outstanding_async_reqs': 0, 'num_remote_worker_restarts': 0}, 'num_env_steps_sampled_lifetime': 240000, 'fault_tolerance': {'num_healthy_workers': 23, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0}, 'num_env_steps_sampled_lifetime_throughput': 1234.2559563699076, 'done': False, 'training_iteration': 60, 'trial_id': 'default', 'date': '2025-04-03_20-01-00', 'timestamp': 1743724860, 'time_this_iter_s': 3.225827932357788, 'time_total_s': 149.10944414138794, 'pid': 72496, 'hostname': 'Nathans-Mac-Studio.local', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'satellite_marl', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 23, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 10, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function <lambda> at 0x36f5314e0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': True, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.01, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'servicer': (None, Box(-inf, inf, (13,), float32), Box(-1.0, 1.0, (6,), float32), None), 'target': (None, Box(-inf, inf, (13,), float32), Box(-1.0, 1.0, (6,), float32), None)}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 149.10944414138794, 'iterations_since_restore': 60, 'perf': {'cpu_util_percent': 76.725, 'ram_util_percent': 56.025}})
2025-04-03 20:01:02,181 [__main__] [INFO] Iter: 61/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 1.99s
2025-04-03 20:01:04,285 [__main__] [INFO] Iter: 62/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.10s
2025-04-03 20:01:06,722 [__main__] [INFO] Iter: 63/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.44s
2025-04-03 20:01:09,039 [__main__] [INFO] Iter: 64/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.32s
2025-04-03 20:01:11,362 [__main__] [INFO] Iter: 65/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.32s
2025-04-03 20:01:13,702 [__main__] [INFO] Iter: 66/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.34s
2025-04-03 20:01:16,075 [__main__] [INFO] Iter: 67/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.37s
2025-04-03 20:01:18,479 [__main__] [INFO] Iter: 68/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.40s
2025-04-03 20:01:20,841 [__main__] [INFO] Iter: 69/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.36s
2025-04-03 20:01:24,159 [__main__] [INFO] Iter: 70/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 3.32s
2025-04-03 20:01:26,210 [__main__] [INFO] Iter: 71/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.05s
2025-04-03 20:01:28,549 [__main__] [INFO] Iter: 72/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.34s
2025-04-03 20:01:30,969 [__main__] [INFO] Iter: 73/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.42s
2025-04-03 20:01:33,385 [__main__] [INFO] Iter: 74/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.42s
2025-04-03 20:01:35,710 [__main__] [INFO] Iter: 75/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.32s
2025-04-03 20:01:37,790 [__main__] [INFO] Iter: 76/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.08s
2025-04-03 20:01:40,209 [__main__] [INFO] Iter: 77/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.42s
2025-04-03 20:01:42,623 [__main__] [INFO] Iter: 78/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.41s
2025-04-03 20:01:45,030 [__main__] [INFO] Iter: 79/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.41s
2025-04-03 20:01:48,252 [__main__] [INFO] Iter: 80/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 3.22s
2025-04-03 20:01:48,274 [__main__] [INFO] Checkpoint saved in directory TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/Users/nathanleo/Downloads/IST 597 - RL/Final Project/RL-Spacecraft-Docking/output/ray_results), metrics={'timers': {'training_iteration': 2.331773132884854, 'restore_env_runners': 1.2444180181548972e-05, 'training_step': 2.331641380783723, 'env_runner_sampling_timer': 0.23229915841810198, 'learner_update_timer': 2.0929342077740314, 'synch_weights': 0.004861582615004543, 'synch_env_connectors': 0.007398921531280501, 'restore_eval_env_runners': 5.385462141050991e-06, 'evaluation_iteration': 3.1062065839150894, 'synch_eval_env_connectors': 0.0029944502541619337}, 'env_runners': {'num_agent_steps_sampled': {'target': 4000, 'servicer': 4000}, 'env_to_module_connector': {'timers': {'connectors': {'numpy_to_tensor': 2.9750022142498313e-05, 'add_observations_from_episodes_to_batch': 1.5980197197438476e-05, 'batch_individual_items': 1.812347944771077e-05, 'add_time_dim_to_batch_and_zero_pad': 6.789053012311867e-06, 'agent_to_module_mapping': 4.097432859596065e-06, 'add_states_from_episodes_to_batch': 4.591390651262104e-06}}, 'connector_pipeline_timer': 0.0001153811477833506}, 'num_agent_steps_sampled_lifetime': {'servicer': 320000, 'target': 320000}, 'num_module_steps_sampled_lifetime': {'target': 320000, 'servicer': 320000}, 'module_to_env_connector': {'timers': {'connectors': {'listify_data_for_vector_env': 9.511425732753429e-06, 'get_actions': 0.00012902600557200666, 'un_batch_to_individual_items': 2.1371781906830904e-05, 'module_to_agent_unmapping': 3.7892105581294635e-06, 'tensor_to_numpy': 4.8159775147428305e-05, 'remove_single_ts_time_rank_from_batch': 1.1525522941579144e-06, 'normalize_and_clip_actions': 5.372721536257657e-05}}, 'connector_pipeline_timer': 0.00031195493401200525}, 'num_module_steps_sampled': {'target': 4000, 'servicer': 4000}, 'timers': {'connectors': {'agent_to_module_mapping': 7.364130199319966e-06, 'add_states_from_episodes_to_batch': 8.177606935572365e-06, 'numpy_to_tensor': 6.412865237191157e-05, 'add_observations_from_episodes_to_batch': 4.1266348410357274e-05, 'batch_individual_items': 3.793108866691751e-05, 'add_time_dim_to_batch_and_zero_pad': 2.1670348992895175e-05}}, 'num_env_steps_sampled_lifetime': 320000, 'num_env_steps_sampled': 4000, 'connector_pipeline_timer': 0.0003306576083717949, 'env_reset_timer': 0.0001808261307735887, 'env_to_module_sum_episodes_length_out': 106.95241103664021, 'sample': 0.15025880474235512, 'env_step_timer': 0.00010233383555598186, 'env_to_module_sum_episodes_length_in': 106.95241103664021, 'rlmodule_inference_timer': 0.00014781793078837353, 'weights_seq_no': 79.0, 'time_between_sampling': 2.1941144708127616, 'episode_len_min': 1000, 'module_episode_returns_mean': {'target': 0.0, 'servicer': -1069.7057858911553}, 'agent_episode_returns_mean': {'target': 0.0, 'servicer': -1069.7057858911553}, 'agent_steps': {'servicer': 1000.0, 'target': 1000.0}, 'episode_return_min': -1085.6922900169905, 'episode_duration_sec_mean': 0.8415339112221061, 'episode_len_mean': 1000.0, 'num_episodes_lifetime': 299, 'episode_return_max': -1053.7334031376076, 'episode_len_max': 1000, 'num_episodes': 0, 'episode_return_mean': -1069.7057858911553, 'num_env_steps_sampled_lifetime_throughput': 1241.546619456604}, 'learners': {'servicer': {'vf_loss_unclipped': 3491.559814453125, 'num_module_steps_trained': 40320, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'curr_entropy_coeff': 0.01, 'policy_loss': 0.017442621290683746, 'curr_kl_coeff': 8.88178432935015e-17, 'gradients_default_optimizer_global_norm': 1.1449207067489624, 'total_loss': 9.46567440032959, 'vf_loss': 9.549888610839844, 'vf_explained_var': 5.334615707397461e-05, 'mean_kl_loss': 0.004615255165845156, 'module_train_batch_size_mean': 128.0, 'entropy': 10.165647506713867, 'num_module_steps_trained_lifetime': 3228288, 'num_trainable_parameters': 142093.0, 'default_optimizer_learning_rate': 5e-05, 'weights_seq_no': 80.0}, 'target': {'default_optimizer_learning_rate': 5e-05, 'weights_seq_no': 80.0, 'num_module_steps_trained': 40320, 'vf_loss_unclipped': 3.6254772339816554e-07, 'curr_entropy_coeff': 0.01, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'policy_loss': 0.013434914872050285, 'curr_kl_coeff': 0.016894055530428886, 'gradients_default_optimizer_global_norm': 0.9261921048164368, 'total_loss': -0.4629274010658264, 'vf_loss': 3.6254772339816554e-07, 'mean_kl_loss': 0.006273150909692049, 'vf_explained_var': 0.9999542236328125, 'module_train_batch_size_mean': 128.0, 'entropy': 47.64686584472656, 'num_module_steps_trained_lifetime': 3228288, 'num_trainable_parameters': 142093.0}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'general_advantage_estimation': 0.02379060911784943, 'add_states_from_episodes_to_batch': 5.13290484259404e-06, 'numpy_to_tensor': 0.000149641815328129, 'add_observations_from_episodes_to_batch': 0.0002239488948093022, 'add_one_ts_to_episodes_and_truncate': 0.004996580894016914, 'batch_individual_items': 0.06442829792128558, 'add_time_dim_to_batch_and_zero_pad': 1.8966425200053838e-05, 'add_columns_from_episodes_to_train_batch': 0.06818929793177914, 'agent_to_module_mapping': 0.003464164474765203}}, 'connector_pipeline_timer': 0.165446033641337}, 'num_module_steps_trained': 80640, 'learner_connector_sum_episodes_length_out': 4000.0, 'num_non_trainable_parameters': 0.0, 'num_env_steps_trained': 1260000, 'learner_connector_sum_episodes_length_in': 4000.0, 'num_env_steps_trained_lifetime': 100884000, 'num_module_steps_trained_lifetime': 6456576, 'num_trainable_parameters': 284186.0, 'num_env_steps_trained_lifetime_throughput': 0.0}}, 'num_training_step_calls_per_iteration': 1, 'evaluation': {'env_runners': {'num_agent_steps_sampled': {'target': 5000, 'servicer': 5000}, 'env_to_module_connector': {'timers': {'connectors': {'numpy_to_tensor': 2.067625236220838e-05, 'add_observations_from_episodes_to_batch': 1.1789152616283489e-05, 'batch_individual_items': 1.2840947209592847e-05, 'add_time_dim_to_batch_and_zero_pad': 4.5783873839657e-06, 'agent_to_module_mapping': 3.0380284178601133e-06, 'add_states_from_episodes_to_batch': 3.014149063831474e-06}}, 'connector_pipeline_timer': 8.318858318526698e-05}, 'num_agent_steps_sampled_lifetime': {'servicer': 40000, 'target': 40000}, 'num_module_steps_sampled_lifetime': {'target': 40000, 'servicer': 40000}, 'module_to_env_connector': {'timers': {'connectors': {'listify_data_for_vector_env': 7.85620290955585e-06, 'get_actions': 4.0502384880676123e-05, 'un_batch_to_individual_items': 1.561377527997412e-05, 'module_to_agent_unmapping': 2.623079801873684e-06, 'tensor_to_numpy': 3.796287802611348e-05, 'remove_single_ts_time_rank_from_batch': 8.554921582694547e-07, 'normalize_and_clip_actions': 4.279183741316252e-05}}, 'connector_pipeline_timer': 0.00018231558192819723}, 'episode_len_min': 1000, 'module_episode_returns_mean': {'target': 0.0, 'servicer': -1026.7133356196596}, 'num_module_steps_sampled': {'target': 5000, 'servicer': 5000}, 'timers': {'connectors': {'agent_to_module_mapping': 6.00069678244867e-06, 'add_states_from_episodes_to_batch': 5.079931840646912e-06, 'numpy_to_tensor': 3.381237420237637e-05, 'add_observations_from_episodes_to_batch': 3.268447613684965e-05, 'batch_individual_items': 2.6985613514923164e-05, 'add_time_dim_to_batch_and_zero_pad': 2.0388714802683686e-05}}, 'num_env_steps_sampled_lifetime': 40000, 'agent_episode_returns_mean': {'target': 0.0, 'servicer': -1026.7133356196596}, 'agent_steps': {'servicer': 1000.0, 'target': 1000.0}, 'episode_return_min': -1044.812060014699, 'episode_duration_sec_mean': 0.5415639220798039, 'episode_len_mean': 1000.0, 'num_episodes_lifetime': 40, 'episode_return_max': -1000.4580394323609, 'num_env_steps_sampled': 5000, 'connector_pipeline_timer': 0.00024008959532589622, 'env_reset_timer': 0.00016340545769963447, 'episode_len_max': 1000, 'env_to_module_sum_episodes_length_out': 901.0423566894266, 'num_episodes': 5, 'sample': 2.9322197825688066, 'env_step_timer': 8.499842137528098e-05, 'env_to_module_sum_episodes_length_in': 901.0423566894266, 'episode_return_mean': -1026.7133356196596, 'rlmodule_inference_timer': 0.00012052015394816291, 'weights_seq_no': 79.0, 'num_env_steps_sampled_per_second': 1604.9822570964222, 'time_between_sampling': 22.10059466634021}, 'num_healthy_workers': 1, 'actor_manager_num_outstanding_async_reqs': 0, 'num_remote_worker_restarts': 0}, 'num_env_steps_sampled_lifetime': 320000, 'fault_tolerance': {'num_healthy_workers': 23, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0}, 'num_env_steps_sampled_lifetime_throughput': 1241.546619456604, 'done': False, 'training_iteration': 80, 'trial_id': 'default', 'date': '2025-04-03_20-01-48', 'timestamp': 1743724908, 'time_this_iter_s': 3.208144187927246, 'time_total_s': 196.89599227905273, 'pid': 72496, 'hostname': 'Nathans-Mac-Studio.local', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'satellite_marl', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 23, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 10, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function <lambda> at 0x36f5314e0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': True, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.01, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'servicer': (None, Box(-inf, inf, (13,), float32), Box(-1.0, 1.0, (6,), float32), None), 'target': (None, Box(-inf, inf, (13,), float32), Box(-1.0, 1.0, (6,), float32), None)}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 196.89599227905273, 'iterations_since_restore': 80, 'perf': {'cpu_util_percent': 79.88000000000001, 'ram_util_percent': 56.1}})
2025-04-03 20:01:50,266 [__main__] [INFO] Iter: 81/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 1.99s
2025-04-03 20:01:52,495 [__main__] [INFO] Iter: 82/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.23s
2025-04-03 20:01:54,922 [__main__] [INFO] Iter: 83/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.43s
2025-04-03 20:01:57,410 [__main__] [INFO] Iter: 84/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.49s
2025-04-03 20:01:59,925 [__main__] [INFO] Iter: 85/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.51s
2025-04-03 20:02:02,345 [__main__] [INFO] Iter: 86/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.42s
2025-04-03 20:02:04,527 [__main__] [INFO] Iter: 87/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.18s
2025-04-03 20:02:06,890 [__main__] [INFO] Iter: 88/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.36s
2025-04-03 20:02:09,336 [__main__] [INFO] Iter: 89/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.45s
2025-04-03 20:02:12,556 [__main__] [INFO] Iter: 90/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 3.22s
2025-04-03 20:02:14,925 [__main__] [INFO] Iter: 91/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.37s
2025-04-03 20:02:17,367 [__main__] [INFO] Iter: 92/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.44s
2025-04-03 20:02:19,714 [__main__] [INFO] Iter: 93/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.35s
2025-04-03 20:02:21,926 [__main__] [INFO] Iter: 94/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.21s
2025-04-03 20:02:24,181 [__main__] [INFO] Iter: 95/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.25s
2025-04-03 20:02:26,550 [__main__] [INFO] Iter: 96/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.37s
2025-04-03 20:02:29,075 [__main__] [INFO] Iter: 97/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.52s
2025-04-03 20:02:31,492 [__main__] [INFO] Iter: 98/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 2.42s
2025-04-03 20:02:33,340 [__main__] [INFO] Iter: 99/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 1.85s
2025-04-03 20:02:36,747 [__main__] [INFO] Iter: 100/100, Ts: 0, Reward (Eval/Sample): nan, Policy Loss: nan, Time: 3.41s
2025-04-03 20:02:36,766 [__main__] [INFO] Checkpoint saved in directory TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/Users/nathanleo/Downloads/IST 597 - RL/Final Project/RL-Spacecraft-Docking/output/ray_results), metrics={'timers': {'training_iteration': 2.3442317097602117, 'restore_env_runners': 1.2384179666922528e-05, 'training_step': 2.34408661542178, 'env_runner_sampling_timer': 0.22921438228474333, 'learner_update_timer': 2.108432340494894, 'synch_weights': 0.004830123071362909, 'synch_env_connectors': 0.007381118972427721, 'restore_eval_env_runners': 5.423255512466166e-06, 'evaluation_iteration': 3.101821698301101, 'synch_eval_env_connectors': 0.0029666152126896234}, 'env_runners': {'num_agent_steps_sampled': {'target': 4000, 'servicer': 4000}, 'env_to_module_connector': {'timers': {'connectors': {'numpy_to_tensor': 2.9362848378705308e-05, 'add_observations_from_episodes_to_batch': 1.5462257639514542e-05, 'batch_individual_items': 1.7859783237777545e-05, 'add_time_dim_to_batch_and_zero_pad': 6.399761071642465e-06, 'agent_to_module_mapping': 4.065119653000191e-06, 'add_states_from_episodes_to_batch': 4.062170807549231e-06}}, 'connector_pipeline_timer': 0.00011294210016592632}, 'num_agent_steps_sampled_lifetime': {'servicer': 400000, 'target': 400000}, 'num_module_steps_sampled_lifetime': {'target': 400000, 'servicer': 400000}, 'module_to_env_connector': {'timers': {'connectors': {'listify_data_for_vector_env': 9.556406441703674e-06, 'get_actions': 0.00012850757660034376, 'un_batch_to_individual_items': 2.1033037726210153e-05, 'module_to_agent_unmapping': 3.7170822187052295e-06, 'tensor_to_numpy': 4.8672129041942046e-05, 'remove_single_ts_time_rank_from_batch': 1.2233368149699456e-06, 'normalize_and_clip_actions': 5.2062551597810184e-05}}, 'connector_pipeline_timer': 0.0003085034178724003}, 'num_module_steps_sampled': {'target': 4000, 'servicer': 4000}, 'timers': {'connectors': {'agent_to_module_mapping': 7.364130199319966e-06, 'add_states_from_episodes_to_batch': 8.177606935572365e-06, 'numpy_to_tensor': 6.412865237191157e-05, 'add_observations_from_episodes_to_batch': 4.1266348410357274e-05, 'batch_individual_items': 3.793108866691751e-05, 'add_time_dim_to_batch_and_zero_pad': 2.1670348992895175e-05}}, 'num_env_steps_sampled_lifetime': 400000, 'num_env_steps_sampled': 4000, 'connector_pipeline_timer': 0.0003306576083717949, 'env_reset_timer': 0.0001808261307735887, 'env_to_module_sum_episodes_length_out': 101.95413642173257, 'sample': 0.1523252325068117, 'env_step_timer': 0.00010048385494869049, 'env_to_module_sum_episodes_length_in': 101.95413642173257, 'rlmodule_inference_timer': 0.00014472307951410933, 'weights_seq_no': 99.0, 'time_between_sampling': 2.2076209751375337, 'episode_len_min': 1000, 'module_episode_returns_mean': {'target': 0.0, 'servicer': -1078.0129581349859}, 'agent_episode_returns_mean': {'target': 0.0, 'servicer': -1078.0129581349859}, 'agent_steps': {'servicer': 1000.0, 'target': 1000.0}, 'episode_return_min': -1088.6841190406467, 'episode_duration_sec_mean': 0.8874124203474544, 'episode_len_mean': 1000.0, 'num_episodes_lifetime': 391, 'episode_return_max': -1066.0511874534611, 'episode_len_max': 1000, 'num_episodes': 0, 'episode_return_mean': -1078.0129581349859, 'num_env_steps_sampled_lifetime_throughput': 1169.7859640090965}, 'learners': {'servicer': {'vf_loss_unclipped': 3439.18408203125, 'num_module_steps_trained': 40320, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'curr_entropy_coeff': 0.01, 'policy_loss': 0.011785965412855148, 'curr_kl_coeff': 1.387778801460961e-18, 'gradients_default_optimizer_global_norm': 1.3140536546707153, 'total_loss': 9.72841739654541, 'vf_loss': 9.828951835632324, 'vf_explained_var': -8.225440979003906e-06, 'mean_kl_loss': 0.005624065641313791, 'module_train_batch_size_mean': 128.0, 'entropy': 11.232147216796875, 'num_module_steps_trained_lifetime': 4035456, 'num_trainable_parameters': 142093.0, 'default_optimizer_learning_rate': 5e-05, 'weights_seq_no': 100.0}, 'target': {'default_optimizer_learning_rate': 5e-05, 'weights_seq_no': 100.0, 'num_module_steps_trained': 40320, 'vf_loss_unclipped': 2.2719910930391052e-07, 'curr_entropy_coeff': 0.01, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'policy_loss': 0.010291457176208496, 'curr_kl_coeff': 0.06414461135864258, 'gradients_default_optimizer_global_norm': 0.33018118143081665, 'total_loss': -0.5158258080482483, 'vf_loss': 2.2719910930391052e-07, 'mean_kl_loss': 0.0094731654971838, 'vf_explained_var': 0.9998993277549744, 'module_train_batch_size_mean': 128.0, 'entropy': 52.672515869140625, 'num_module_steps_trained_lifetime': 4035456, 'num_trainable_parameters': 142093.0}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'general_advantage_estimation': 0.022835689333514354, 'add_states_from_episodes_to_batch': 5.1388654157919085e-06, 'numpy_to_tensor': 0.00014788288369121407, 'add_observations_from_episodes_to_batch': 0.00022721619863058415, 'add_one_ts_to_episodes_and_truncate': 0.0050094149812214325, 'batch_individual_items': 0.06589238848298994, 'add_time_dim_to_batch_and_zero_pad': 1.8653611038050235e-05, 'add_columns_from_episodes_to_train_batch': 0.06842301042147587, 'agent_to_module_mapping': 0.003490035802703135}}, 'connector_pipeline_timer': 0.1662232097202317}, 'num_module_steps_trained': 80640, 'learner_connector_sum_episodes_length_out': 4000.0, 'num_non_trainable_parameters': 0.0, 'num_env_steps_trained': 1260000, 'learner_connector_sum_episodes_length_in': 4000.0, 'num_env_steps_trained_lifetime': 126108000, 'num_module_steps_trained_lifetime': 8070912, 'num_trainable_parameters': 284186.0, 'num_env_steps_trained_lifetime_throughput': 0.0}}, 'num_training_step_calls_per_iteration': 1, 'evaluation': {'env_runners': {'num_agent_steps_sampled': {'target': 5000, 'servicer': 5000}, 'env_to_module_connector': {'timers': {'connectors': {'numpy_to_tensor': 2.0617493464965104e-05, 'add_observations_from_episodes_to_batch': 1.175864858808681e-05, 'batch_individual_items': 1.2812804466246893e-05, 'add_time_dim_to_batch_and_zero_pad': 4.564956436786594e-06, 'agent_to_module_mapping': 3.032898472788447e-06, 'add_states_from_episodes_to_batch': 3.010957104399126e-06}}, 'connector_pipeline_timer': 8.300267027171614e-05}, 'num_agent_steps_sampled_lifetime': {'servicer': 50000, 'target': 50000}, 'num_module_steps_sampled_lifetime': {'target': 50000, 'servicer': 50000}, 'module_to_env_connector': {'timers': {'connectors': {'listify_data_for_vector_env': 7.831417834980931e-06, 'get_actions': 4.036310471879847e-05, 'un_batch_to_individual_items': 1.5581902813527996e-05, 'module_to_agent_unmapping': 2.6178559853712747e-06, 'tensor_to_numpy': 3.785045056549451e-05, 'remove_single_ts_time_rank_from_batch': 8.579697334757007e-07, 'normalize_and_clip_actions': 4.263197722770472e-05}}, 'connector_pipeline_timer': 0.00018177244137162525}, 'episode_len_min': 1000, 'module_episode_returns_mean': {'target': 0.0, 'servicer': -1041.6820473226712}, 'num_module_steps_sampled': {'target': 5000, 'servicer': 5000}, 'timers': {'connectors': {'agent_to_module_mapping': 5.999403642400649e-06, 'add_states_from_episodes_to_batch': 5.077867677577988e-06, 'numpy_to_tensor': 3.383336682822211e-05, 'add_observations_from_episodes_to_batch': 3.2669778734109445e-05, 'batch_individual_items': 2.6976393665629375e-05, 'add_time_dim_to_batch_and_zero_pad': 2.0371518084633247e-05}}, 'num_env_steps_sampled_lifetime': 50000, 'agent_episode_returns_mean': {'target': 0.0, 'servicer': -1041.6820473226712}, 'agent_steps': {'servicer': 1000.0, 'target': 1000.0}, 'episode_return_min': -1259.1969525562856, 'episode_duration_sec_mean': 0.5391846812504809, 'episode_len_mean': 1000.0, 'num_episodes_lifetime': 50, 'episode_return_max': -1000.4580394323609, 'num_env_steps_sampled': 5000, 'connector_pipeline_timer': 0.00023994726324355292, 'env_reset_timer': 0.00016332144751109357, 'episode_len_max': 1000, 'env_to_module_sum_episodes_length_out': 901.0423566894266, 'num_episodes': 5, 'sample': 2.9319494658237972, 'env_step_timer': 8.473537799256068e-05, 'env_to_module_sum_episodes_length_in': 901.0423566894266, 'episode_return_mean': -1041.6820473226712, 'rlmodule_inference_timer': 0.00012014089204329261, 'weights_seq_no': 99.0, 'num_env_steps_sampled_per_second': 1605.0893225544987, 'time_between_sampling': 22.100298200153293}, 'num_healthy_workers': 1, 'actor_manager_num_outstanding_async_reqs': 0, 'num_remote_worker_restarts': 0}, 'num_env_steps_sampled_lifetime': 400000, 'fault_tolerance': {'num_healthy_workers': 23, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0}, 'num_env_steps_sampled_lifetime_throughput': 1169.7859640090965, 'done': False, 'training_iteration': 100, 'trial_id': 'default', 'date': '2025-04-03_20-02-36', 'timestamp': 1743724956, 'time_this_iter_s': 3.3926138877868652, 'time_total_s': 245.05060648918152, 'pid': 72496, 'hostname': 'Nathans-Mac-Studio.local', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'satellite_marl', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 23, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 10, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function <lambda> at 0x36f5314e0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': True, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.01, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'servicer': (None, Box(-inf, inf, (13,), float32), Box(-1.0, 1.0, (6,), float32), None), 'target': (None, Box(-inf, inf, (13,), float32), Box(-1.0, 1.0, (6,), float32), None)}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 245.05060648918152, 'iterations_since_restore': 100, 'perf': {'cpu_util_percent': 80.82000000000001, 'ram_util_percent': 56.1}})
2025-04-03 20:02:36,767 [__main__] [INFO] 
--- Training Finished ---
2025-04-03 20:02:36,767 [__main__] [INFO] Total Training Time: 246.61 seconds
2025-04-03 20:02:36,767 [__main__] [INFO] Using last saved checkpoint: TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/Users/nathanleo/Downloads/IST 597 - RL/Final Project/RL-Spacecraft-Docking/output/ray_results), metrics={'timers': {'training_iteration': 2.3442317097602117, 'restore_env_runners': 1.2384179666922528e-05, 'training_step': 2.34408661542178, 'env_runner_sampling_timer': 0.22921438228474333, 'learner_update_timer': 2.108432340494894, 'synch_weights': 0.004830123071362909, 'synch_env_connectors': 0.007381118972427721, 'restore_eval_env_runners': 5.423255512466166e-06, 'evaluation_iteration': 3.101821698301101, 'synch_eval_env_connectors': 0.0029666152126896234}, 'env_runners': {'num_agent_steps_sampled': {'target': 4000, 'servicer': 4000}, 'env_to_module_connector': {'timers': {'connectors': {'numpy_to_tensor': 2.9362848378705308e-05, 'add_observations_from_episodes_to_batch': 1.5462257639514542e-05, 'batch_individual_items': 1.7859783237777545e-05, 'add_time_dim_to_batch_and_zero_pad': 6.399761071642465e-06, 'agent_to_module_mapping': 4.065119653000191e-06, 'add_states_from_episodes_to_batch': 4.062170807549231e-06}}, 'connector_pipeline_timer': 0.00011294210016592632}, 'num_agent_steps_sampled_lifetime': {'servicer': 400000, 'target': 400000}, 'num_module_steps_sampled_lifetime': {'target': 400000, 'servicer': 400000}, 'module_to_env_connector': {'timers': {'connectors': {'listify_data_for_vector_env': 9.556406441703674e-06, 'get_actions': 0.00012850757660034376, 'un_batch_to_individual_items': 2.1033037726210153e-05, 'module_to_agent_unmapping': 3.7170822187052295e-06, 'tensor_to_numpy': 4.8672129041942046e-05, 'remove_single_ts_time_rank_from_batch': 1.2233368149699456e-06, 'normalize_and_clip_actions': 5.2062551597810184e-05}}, 'connector_pipeline_timer': 0.0003085034178724003}, 'num_module_steps_sampled': {'target': 4000, 'servicer': 4000}, 'timers': {'connectors': {'agent_to_module_mapping': 7.364130199319966e-06, 'add_states_from_episodes_to_batch': 8.177606935572365e-06, 'numpy_to_tensor': 6.412865237191157e-05, 'add_observations_from_episodes_to_batch': 4.1266348410357274e-05, 'batch_individual_items': 3.793108866691751e-05, 'add_time_dim_to_batch_and_zero_pad': 2.1670348992895175e-05}}, 'num_env_steps_sampled_lifetime': 400000, 'num_env_steps_sampled': 4000, 'connector_pipeline_timer': 0.0003306576083717949, 'env_reset_timer': 0.0001808261307735887, 'env_to_module_sum_episodes_length_out': 101.95413642173257, 'sample': 0.1523252325068117, 'env_step_timer': 0.00010048385494869049, 'env_to_module_sum_episodes_length_in': 101.95413642173257, 'rlmodule_inference_timer': 0.00014472307951410933, 'weights_seq_no': 99.0, 'time_between_sampling': 2.2076209751375337, 'episode_len_min': 1000, 'module_episode_returns_mean': {'target': 0.0, 'servicer': -1078.0129581349859}, 'agent_episode_returns_mean': {'target': 0.0, 'servicer': -1078.0129581349859}, 'agent_steps': {'servicer': 1000.0, 'target': 1000.0}, 'episode_return_min': -1088.6841190406467, 'episode_duration_sec_mean': 0.8874124203474544, 'episode_len_mean': 1000.0, 'num_episodes_lifetime': 391, 'episode_return_max': -1066.0511874534611, 'episode_len_max': 1000, 'num_episodes': 0, 'episode_return_mean': -1078.0129581349859, 'num_env_steps_sampled_lifetime_throughput': 1169.7859640090965}, 'learners': {'servicer': {'vf_loss_unclipped': 3439.18408203125, 'num_module_steps_trained': 40320, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'curr_entropy_coeff': 0.01, 'policy_loss': 0.011785965412855148, 'curr_kl_coeff': 1.387778801460961e-18, 'gradients_default_optimizer_global_norm': 1.3140536546707153, 'total_loss': 9.72841739654541, 'vf_loss': 9.828951835632324, 'vf_explained_var': -8.225440979003906e-06, 'mean_kl_loss': 0.005624065641313791, 'module_train_batch_size_mean': 128.0, 'entropy': 11.232147216796875, 'num_module_steps_trained_lifetime': 4035456, 'num_trainable_parameters': 142093.0, 'default_optimizer_learning_rate': 5e-05, 'weights_seq_no': 100.0}, 'target': {'default_optimizer_learning_rate': 5e-05, 'weights_seq_no': 100.0, 'num_module_steps_trained': 40320, 'vf_loss_unclipped': 2.2719910930391052e-07, 'curr_entropy_coeff': 0.01, 'diff_num_grad_updates_vs_sampler_policy': 1.0, 'policy_loss': 0.010291457176208496, 'curr_kl_coeff': 0.06414461135864258, 'gradients_default_optimizer_global_norm': 0.33018118143081665, 'total_loss': -0.5158258080482483, 'vf_loss': 2.2719910930391052e-07, 'mean_kl_loss': 0.0094731654971838, 'vf_explained_var': 0.9998993277549744, 'module_train_batch_size_mean': 128.0, 'entropy': 52.672515869140625, 'num_module_steps_trained_lifetime': 4035456, 'num_trainable_parameters': 142093.0}, '__all_modules__': {'learner_connector': {'timers': {'connectors': {'general_advantage_estimation': 0.022835689333514354, 'add_states_from_episodes_to_batch': 5.1388654157919085e-06, 'numpy_to_tensor': 0.00014788288369121407, 'add_observations_from_episodes_to_batch': 0.00022721619863058415, 'add_one_ts_to_episodes_and_truncate': 0.0050094149812214325, 'batch_individual_items': 0.06589238848298994, 'add_time_dim_to_batch_and_zero_pad': 1.8653611038050235e-05, 'add_columns_from_episodes_to_train_batch': 0.06842301042147587, 'agent_to_module_mapping': 0.003490035802703135}}, 'connector_pipeline_timer': 0.1662232097202317}, 'num_module_steps_trained': 80640, 'learner_connector_sum_episodes_length_out': 4000.0, 'num_non_trainable_parameters': 0.0, 'num_env_steps_trained': 1260000, 'learner_connector_sum_episodes_length_in': 4000.0, 'num_env_steps_trained_lifetime': 126108000, 'num_module_steps_trained_lifetime': 8070912, 'num_trainable_parameters': 284186.0, 'num_env_steps_trained_lifetime_throughput': 0.0}}, 'num_training_step_calls_per_iteration': 1, 'evaluation': {'env_runners': {'num_agent_steps_sampled': {'target': 5000, 'servicer': 5000}, 'env_to_module_connector': {'timers': {'connectors': {'numpy_to_tensor': 2.0617493464965104e-05, 'add_observations_from_episodes_to_batch': 1.175864858808681e-05, 'batch_individual_items': 1.2812804466246893e-05, 'add_time_dim_to_batch_and_zero_pad': 4.564956436786594e-06, 'agent_to_module_mapping': 3.032898472788447e-06, 'add_states_from_episodes_to_batch': 3.010957104399126e-06}}, 'connector_pipeline_timer': 8.300267027171614e-05}, 'num_agent_steps_sampled_lifetime': {'servicer': 50000, 'target': 50000}, 'num_module_steps_sampled_lifetime': {'target': 50000, 'servicer': 50000}, 'module_to_env_connector': {'timers': {'connectors': {'listify_data_for_vector_env': 7.831417834980931e-06, 'get_actions': 4.036310471879847e-05, 'un_batch_to_individual_items': 1.5581902813527996e-05, 'module_to_agent_unmapping': 2.6178559853712747e-06, 'tensor_to_numpy': 3.785045056549451e-05, 'remove_single_ts_time_rank_from_batch': 8.579697334757007e-07, 'normalize_and_clip_actions': 4.263197722770472e-05}}, 'connector_pipeline_timer': 0.00018177244137162525}, 'episode_len_min': 1000, 'module_episode_returns_mean': {'target': 0.0, 'servicer': -1041.6820473226712}, 'num_module_steps_sampled': {'target': 5000, 'servicer': 5000}, 'timers': {'connectors': {'agent_to_module_mapping': 5.999403642400649e-06, 'add_states_from_episodes_to_batch': 5.077867677577988e-06, 'numpy_to_tensor': 3.383336682822211e-05, 'add_observations_from_episodes_to_batch': 3.2669778734109445e-05, 'batch_individual_items': 2.6976393665629375e-05, 'add_time_dim_to_batch_and_zero_pad': 2.0371518084633247e-05}}, 'num_env_steps_sampled_lifetime': 50000, 'agent_episode_returns_mean': {'target': 0.0, 'servicer': -1041.6820473226712}, 'agent_steps': {'servicer': 1000.0, 'target': 1000.0}, 'episode_return_min': -1259.1969525562856, 'episode_duration_sec_mean': 0.5391846812504809, 'episode_len_mean': 1000.0, 'num_episodes_lifetime': 50, 'episode_return_max': -1000.4580394323609, 'num_env_steps_sampled': 5000, 'connector_pipeline_timer': 0.00023994726324355292, 'env_reset_timer': 0.00016332144751109357, 'episode_len_max': 1000, 'env_to_module_sum_episodes_length_out': 901.0423566894266, 'num_episodes': 5, 'sample': 2.9319494658237972, 'env_step_timer': 8.473537799256068e-05, 'env_to_module_sum_episodes_length_in': 901.0423566894266, 'episode_return_mean': -1041.6820473226712, 'rlmodule_inference_timer': 0.00012014089204329261, 'weights_seq_no': 99.0, 'num_env_steps_sampled_per_second': 1605.0893225544987, 'time_between_sampling': 22.100298200153293}, 'num_healthy_workers': 1, 'actor_manager_num_outstanding_async_reqs': 0, 'num_remote_worker_restarts': 0}, 'num_env_steps_sampled_lifetime': 400000, 'fault_tolerance': {'num_healthy_workers': 23, 'num_remote_worker_restarts': 0}, 'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0}, 'num_env_steps_sampled_lifetime_throughput': 1169.7859640090965, 'done': False, 'training_iteration': 100, 'trial_id': 'default', 'date': '2025-04-03_20-02-36', 'timestamp': 1743724956, 'time_this_iter_s': 3.3926138877868652, 'time_total_s': 245.05060648918152, 'pid': 72496, 'hostname': 'Nathans-Mac-Studio.local', 'node_ip': '127.0.0.1', 'config': {'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'aot_eager', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'aot_eager', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'satellite_marl', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 23, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 10, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': True, 'enable_env_runner_and_connector_v2': True, '_prior_exploration_config': {'type': 'StochasticSampling'}, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function <lambda> at 0x36f5314e0>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 10, 'evaluation_duration': 5, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_parallel_to_training': True, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': {'explore': False}, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 1, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': True, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.01, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 1.0, 'input': 'sampler', 'policies': {'servicer': (None, Box(-inf, inf, (13,), float32), Box(-1.0, 1.0, (6,), float32), None), 'target': (None, Box(-inf, inf, (13,), float32), Box(-1.0, 1.0, (6,), float32), None)}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 245.05060648918152, 'iterations_since_restore': 100, 'perf': {'cpu_util_percent': 80.82000000000001, 'ram_util_percent': 56.1}})
2025-04-03 20:02:36,767 [__main__] [INFO] 
--- Running Evaluation & Recording Video ---
2025-04-03 20:02:36,769 [__main__] [INFO] Successfully retrieved RLModules for evaluation.
2025-04-03 20:02:36,769 [__main__] [INFO] Evaluation Episode: 1/5
2025-04-03 20:02:36,862 [src.satellite_marl_env] [INFO] MuJoCo Renderer initialized.
2025-04-03 20:02:36,875 [__main__] [ERROR] Error during new API action computation in evaluation: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray
Traceback (most recent call last):
  File "/Users/nathanleo/Downloads/IST 597 - RL/Final Project/RL-Spacecraft-Docking/src/train_marl.py", line 158, in run_evaluation_video
    raise te
  File "/Users/nathanleo/Downloads/IST 597 - RL/Final Project/RL-Spacecraft-Docking/src/train_marl.py", line 152, in run_evaluation_video
    forward_outs[agent_id] = module.forward_inference(input_dict)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py", line 564, in forward_inference
    return self._forward_inference(batch, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/rl_module/torch/torch_rl_module.py", line 89, in _forward_inference
    return self._forward(batch, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/torch/default_ppo_torch_rl_module.py", line 31, in _forward
    encoder_outs = self.encoder(batch)
                   ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/models/torch/base.py", line 77, in forward
    return self._forward(inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/models/base.py", line 333, in _forward
    actor_out = self.actor_encoder(inputs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/models/torch/base.py", line 77, in forward
    return self._forward(inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/models/torch/encoder.py", line 79, in _forward
    return {ENCODER_OUT: self.net(inputs[Columns.OBS])}
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/models/torch/primitives.py", line 178, in forward
    return self.mlp(x)
           ^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray
2025-04-03 20:02:36,880 [__main__] [INFO] Evaluation Episode 1 Rewards: {'servicer': 0.0, 'target': 0.0}
2025-04-03 20:02:36,880 [__main__] [INFO] Evaluation Episode: 2/5
2025-04-03 20:02:36,882 [__main__] [ERROR] Error during new API action computation in evaluation: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray
Traceback (most recent call last):
  File "/Users/nathanleo/Downloads/IST 597 - RL/Final Project/RL-Spacecraft-Docking/src/train_marl.py", line 158, in run_evaluation_video
    raise te
  File "/Users/nathanleo/Downloads/IST 597 - RL/Final Project/RL-Spacecraft-Docking/src/train_marl.py", line 152, in run_evaluation_video
    forward_outs[agent_id] = module.forward_inference(input_dict)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py", line 564, in forward_inference
    return self._forward_inference(batch, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/rl_module/torch/torch_rl_module.py", line 89, in _forward_inference
    return self._forward(batch, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/torch/default_ppo_torch_rl_module.py", line 31, in _forward
    encoder_outs = self.encoder(batch)
                   ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/models/torch/base.py", line 77, in forward
    return self._forward(inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/models/base.py", line 333, in _forward
    actor_out = self.actor_encoder(inputs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/models/torch/base.py", line 77, in forward
    return self._forward(inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/models/torch/encoder.py", line 79, in _forward
    return {ENCODER_OUT: self.net(inputs[Columns.OBS])}
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/models/torch/primitives.py", line 178, in forward
    return self.mlp(x)
           ^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray
2025-04-03 20:02:36,883 [__main__] [INFO] Evaluation Episode 2 Rewards: {'servicer': 0.0, 'target': 0.0}
2025-04-03 20:02:36,883 [__main__] [INFO] Evaluation Episode: 3/5
2025-04-03 20:02:36,885 [__main__] [ERROR] Error during new API action computation in evaluation: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray
Traceback (most recent call last):
  File "/Users/nathanleo/Downloads/IST 597 - RL/Final Project/RL-Spacecraft-Docking/src/train_marl.py", line 158, in run_evaluation_video
    raise te
  File "/Users/nathanleo/Downloads/IST 597 - RL/Final Project/RL-Spacecraft-Docking/src/train_marl.py", line 152, in run_evaluation_video
    forward_outs[agent_id] = module.forward_inference(input_dict)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py", line 564, in forward_inference
    return self._forward_inference(batch, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/rl_module/torch/torch_rl_module.py", line 89, in _forward_inference
    return self._forward(batch, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/torch/default_ppo_torch_rl_module.py", line 31, in _forward
    encoder_outs = self.encoder(batch)
                   ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/models/torch/base.py", line 77, in forward
    return self._forward(inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/models/base.py", line 333, in _forward
    actor_out = self.actor_encoder(inputs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/models/torch/base.py", line 77, in forward
    return self._forward(inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/models/torch/encoder.py", line 79, in _forward
    return {ENCODER_OUT: self.net(inputs[Columns.OBS])}
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/models/torch/primitives.py", line 178, in forward
    return self.mlp(x)
           ^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray
2025-04-03 20:02:36,886 [__main__] [INFO] Evaluation Episode 3 Rewards: {'servicer': 0.0, 'target': 0.0}
2025-04-03 20:02:36,886 [__main__] [INFO] Evaluation Episode: 4/5
2025-04-03 20:02:36,888 [__main__] [ERROR] Error during new API action computation in evaluation: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray
Traceback (most recent call last):
  File "/Users/nathanleo/Downloads/IST 597 - RL/Final Project/RL-Spacecraft-Docking/src/train_marl.py", line 158, in run_evaluation_video
    raise te
  File "/Users/nathanleo/Downloads/IST 597 - RL/Final Project/RL-Spacecraft-Docking/src/train_marl.py", line 152, in run_evaluation_video
    forward_outs[agent_id] = module.forward_inference(input_dict)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py", line 564, in forward_inference
    return self._forward_inference(batch, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/rl_module/torch/torch_rl_module.py", line 89, in _forward_inference
    return self._forward(batch, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/torch/default_ppo_torch_rl_module.py", line 31, in _forward
    encoder_outs = self.encoder(batch)
                   ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/models/torch/base.py", line 77, in forward
    return self._forward(inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/models/base.py", line 333, in _forward
    actor_out = self.actor_encoder(inputs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/models/torch/base.py", line 77, in forward
    return self._forward(inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/models/torch/encoder.py", line 79, in _forward
    return {ENCODER_OUT: self.net(inputs[Columns.OBS])}
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/models/torch/primitives.py", line 178, in forward
    return self.mlp(x)
           ^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray
2025-04-03 20:02:36,888 [__main__] [INFO] Evaluation Episode 4 Rewards: {'servicer': 0.0, 'target': 0.0}
2025-04-03 20:02:36,888 [__main__] [INFO] Evaluation Episode: 5/5
2025-04-03 20:02:36,890 [__main__] [ERROR] Error during new API action computation in evaluation: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray
Traceback (most recent call last):
  File "/Users/nathanleo/Downloads/IST 597 - RL/Final Project/RL-Spacecraft-Docking/src/train_marl.py", line 158, in run_evaluation_video
    raise te
  File "/Users/nathanleo/Downloads/IST 597 - RL/Final Project/RL-Spacecraft-Docking/src/train_marl.py", line 152, in run_evaluation_video
    forward_outs[agent_id] = module.forward_inference(input_dict)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/rl_module/rl_module.py", line 564, in forward_inference
    return self._forward_inference(batch, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/rl_module/torch/torch_rl_module.py", line 89, in _forward_inference
    return self._forward(batch, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/algorithms/ppo/torch/default_ppo_torch_rl_module.py", line 31, in _forward
    encoder_outs = self.encoder(batch)
                   ^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/models/torch/base.py", line 77, in forward
    return self._forward(inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/models/base.py", line 333, in _forward
    actor_out = self.actor_encoder(inputs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/models/torch/base.py", line 77, in forward
    return self._forward(inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/models/torch/encoder.py", line 79, in _forward
    return {ENCODER_OUT: self.net(inputs[Columns.OBS])}
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/ray/rllib/core/models/torch/primitives.py", line 178, in forward
    return self.mlp(x)
           ^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray
2025-04-03 20:02:36,891 [__main__] [INFO] Evaluation Episode 5 Rewards: {'servicer': 0.0, 'target': 0.0}
2025-04-03 20:02:36,899 [src.satellite_marl_env] [INFO] SatelliteMARLEnv closed.
2025-04-03 20:02:36,899 [__main__] [INFO] Average Evaluation Rewards over 5 episodes: {'servicer': 0.0, 'target': 0.0}
2025-04-03 20:02:36,899 [__main__] [INFO] Saving evaluation video to: /Users/nathanleo/Downloads/IST 597 - RL/Final Project/RL-Spacecraft-Docking/output/ray_results/evaluation_video.mp4
2025-04-03 20:02:37,068 [__main__] [INFO] Shutting down Ray...
2025-04-03 20:02:37,416 [src.satellite_marl_env] [INFO] SatelliteMARLEnv closed.
2025-04-03 20:02:37,439 [src.satellite_marl_env] [INFO] SatelliteMARLEnv closed.
2025-04-03 20:02:39,094 [__main__] [INFO] Script finished.
