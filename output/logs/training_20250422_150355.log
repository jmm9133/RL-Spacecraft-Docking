2025-04-22 15:03:55,788 [__main__] [INFO] Logging setup complete. Log file: output/logs/training_20250422_150355.log
2025-04-22 15:03:55,789 [__main__] [INFO] Registered RLlib environment 'satellite_marl'
2025-04-22 15:03:55,789 [__main__] [INFO] --- Starting MARL Training Script ---
2025-04-22 15:03:55,790 [__main__] [INFO] Environment Config: src.config
2025-04-22 15:03:55,790 [__main__] [INFO] RLlib PPO Algorithm
2025-04-22 15:03:55,790 [__main__] [INFO] Training Iterations: 100
2025-04-22 15:03:55,790 [__main__] [INFO] Results Directory: output/ray_results
2025-04-22 15:03:55,790 [__main__] [INFO] Detected 16 CPUs.
2025-04-22 15:03:55,880 [filelock] [DEBUG] Attempting to acquire lock 5870536976 on /tmp/ray/session_2025-04-22_15-03-55_878603_79425/node_ip_address.json.lock
2025-04-22 15:03:55,880 [filelock] [DEBUG] Lock 5870536976 acquired on /tmp/ray/session_2025-04-22_15-03-55_878603_79425/node_ip_address.json.lock
2025-04-22 15:03:55,881 [filelock] [DEBUG] Attempting to release lock 5870536976 on /tmp/ray/session_2025-04-22_15-03-55_878603_79425/node_ip_address.json.lock
2025-04-22 15:03:55,881 [filelock] [DEBUG] Lock 5870536976 released on /tmp/ray/session_2025-04-22_15-03-55_878603_79425/node_ip_address.json.lock
2025-04-22 15:03:55,883 [filelock] [DEBUG] Attempting to acquire lock 4975392464 on /tmp/ray/session_2025-04-22_15-03-55_878603_79425/ports_by_node.json.lock
2025-04-22 15:03:55,883 [filelock] [DEBUG] Lock 4975392464 acquired on /tmp/ray/session_2025-04-22_15-03-55_878603_79425/ports_by_node.json.lock
2025-04-22 15:03:55,884 [filelock] [DEBUG] Attempting to release lock 4975392464 on /tmp/ray/session_2025-04-22_15-03-55_878603_79425/ports_by_node.json.lock
2025-04-22 15:03:55,884 [filelock] [DEBUG] Lock 4975392464 released on /tmp/ray/session_2025-04-22_15-03-55_878603_79425/ports_by_node.json.lock
2025-04-22 15:03:55,884 [filelock] [DEBUG] Attempting to acquire lock 5408457296 on /tmp/ray/session_2025-04-22_15-03-55_878603_79425/ports_by_node.json.lock
2025-04-22 15:03:55,885 [filelock] [DEBUG] Lock 5408457296 acquired on /tmp/ray/session_2025-04-22_15-03-55_878603_79425/ports_by_node.json.lock
2025-04-22 15:03:55,886 [filelock] [DEBUG] Attempting to release lock 5408457296 on /tmp/ray/session_2025-04-22_15-03-55_878603_79425/ports_by_node.json.lock
2025-04-22 15:03:55,886 [filelock] [DEBUG] Lock 5408457296 released on /tmp/ray/session_2025-04-22_15-03-55_878603_79425/ports_by_node.json.lock
2025-04-22 15:03:55,886 [filelock] [DEBUG] Attempting to acquire lock 4975392464 on /tmp/ray/session_2025-04-22_15-03-55_878603_79425/ports_by_node.json.lock
2025-04-22 15:03:55,887 [filelock] [DEBUG] Lock 4975392464 acquired on /tmp/ray/session_2025-04-22_15-03-55_878603_79425/ports_by_node.json.lock
2025-04-22 15:03:55,888 [filelock] [DEBUG] Attempting to release lock 4975392464 on /tmp/ray/session_2025-04-22_15-03-55_878603_79425/ports_by_node.json.lock
2025-04-22 15:03:55,888 [filelock] [DEBUG] Lock 4975392464 released on /tmp/ray/session_2025-04-22_15-03-55_878603_79425/ports_by_node.json.lock
2025-04-22 15:03:55,888 [filelock] [DEBUG] Attempting to acquire lock 5868764880 on /tmp/ray/session_2025-04-22_15-03-55_878603_79425/ports_by_node.json.lock
2025-04-22 15:03:55,889 [filelock] [DEBUG] Lock 5868764880 acquired on /tmp/ray/session_2025-04-22_15-03-55_878603_79425/ports_by_node.json.lock
2025-04-22 15:03:55,890 [filelock] [DEBUG] Attempting to release lock 5868764880 on /tmp/ray/session_2025-04-22_15-03-55_878603_79425/ports_by_node.json.lock
2025-04-22 15:03:55,890 [filelock] [DEBUG] Lock 5868764880 released on /tmp/ray/session_2025-04-22_15-03-55_878603_79425/ports_by_node.json.lock
2025-04-22 15:03:55,890 [filelock] [DEBUG] Attempting to acquire lock 5870537104 on /tmp/ray/session_2025-04-22_15-03-55_878603_79425/ports_by_node.json.lock
2025-04-22 15:03:55,891 [filelock] [DEBUG] Lock 5870537104 acquired on /tmp/ray/session_2025-04-22_15-03-55_878603_79425/ports_by_node.json.lock
2025-04-22 15:03:55,892 [filelock] [DEBUG] Attempting to release lock 5870537104 on /tmp/ray/session_2025-04-22_15-03-55_878603_79425/ports_by_node.json.lock
2025-04-22 15:03:55,892 [filelock] [DEBUG] Lock 5870537104 released on /tmp/ray/session_2025-04-22_15-03-55_878603_79425/ports_by_node.json.lock
2025-04-22 15:03:59,312 [__main__] [INFO] Ray initialized successfully.
2025-04-22 15:03:59,312 [__main__] [INFO] Creating temporary environment to get action/observation spaces...
2025-04-22 15:03:59,313 [__main__] [DEBUG] Creating RllibSatelliteEnv with config: {}
2025-04-22 15:03:59,317 [src.satellite_marl_env] [INFO] MuJoCo model loaded successfully from /Users/leo/RL-Spacecraft-Docking/src/xml_references/satellites.xml
2025-04-22 15:03:59,318 [src.satellite_marl_env] [DEBUG] MuJoCo IDs retrieved successfully.
2025-04-22 15:03:59,319 [src.satellite_marl_env] [INFO] SatelliteMARLEnv closed.
2025-04-22 15:03:59,319 [__main__] [INFO] Observation/Action spaces retrieved successfully:
2025-04-22 15:03:59,319 [__main__] [INFO]   Agent 'servicer': Obs=Box(-inf, inf, (13,), float32), Act=Box(-1.0, 1.0, (6,), float32)
2025-04-22 15:03:59,320 [__main__] [INFO]   Agent 'target': Obs=Box(-inf, inf, (13,), float32), Act=Box(-1.0, 1.0, (6,), float32)
2025-04-22 15:03:59,320 [__main__] [INFO] Configuring RLlib PPO Algorithm...
2025-04-22 15:03:59,320 [__main__] [INFO] Absolute results path: /Users/leo/RL-Spacecraft-Docking/output/ray_results
2025-04-22 15:03:59,320 [__main__] [INFO] Using 14 environment runners (workers).
2025-04-22 15:03:59,320 [__main__] [INFO] Setting rollout_fragment_length: 200
2025-04-22 15:03:59,320 [__main__] [INFO] Setting train_batch_size: 2800 (= num_workers * rollout_fragment_length)
2025-04-22 15:03:59,323 [__main__] [INFO] PPO configuration object created.
2025-04-22 15:03:59,323 [__main__] [INFO] Building Algorithm...
2025-04-22 15:04:09,613 [__main__] [DEBUG] Creating RllibSatelliteEnv with config: {}
2025-04-22 15:04:09,616 [src.satellite_marl_env] [INFO] MuJoCo model loaded successfully from /Users/leo/RL-Spacecraft-Docking/src/xml_references/satellites.xml
2025-04-22 15:04:09,617 [src.satellite_marl_env] [DEBUG] MuJoCo IDs retrieved successfully.
2025-04-22 15:04:09,618 [src.satellite_marl_env] [DEBUG] --- Environment Reset Called ---
2025-04-22 15:04:09,618 [src.satellite_marl_env] [DEBUG] Resetting with seed: 42
2025-04-22 15:04:09,618 [src.satellite_marl_env] [DEBUG] MuJoCo data reset.
2025-04-22 15:04:09,618 [src.satellite_marl_env] [DEBUG] Reset: Initial Servicer qpos[0:7]: [0. 0. 0. 1. 0. 0. 0.]
2025-04-22 15:04:09,619 [src.satellite_marl_env] [DEBUG] Reset: Initial Target qpos[0:7]: [2.  0.5 0.  1.  0.  0.  0. ]
2025-04-22 15:04:09,619 [src.satellite_marl_env] [DEBUG] Reset: Initial Servicer qvel[0:6]: [0. 0. 0. 0. 0. 0.]
2025-04-22 15:04:09,619 [src.satellite_marl_env] [DEBUG] Reset: Initial Target qvel[0:6]: [0. 0. 0. 0. 0. 0.]
2025-04-22 15:04:09,619 [src.satellite_marl_env] [DEBUG] Initial mj_forward() completed.
2025-04-22 15:04:09,620 [src.satellite_marl_env] [DEBUG] Reset: Initial Obs 'servicer' (finite: True): [2.  0.5 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0. ]
2025-04-22 15:04:09,620 [src.satellite_marl_env] [DEBUG] Reset: Initial Obs 'target' (finite: True): [-2.  -0.5 -0.  -0.  -0.  -0.   1.   0.   0.   0.   0.   0.   0. ]
2025-04-22 15:04:09,622 [src.satellite_marl_env] [DEBUG] Reset: Initial Docking Dist: 2.1471, Rel Vel Mag: 0.0000
2025-04-22 15:04:09,622 [src.satellite_marl_env] [DEBUG] Reset: Initial Collision Detected: False (ncon=0)
2025-04-22 15:04:09,622 [src.satellite_marl_env] [DEBUG] Reset: Initialized prev_docking_distance = 2.1471
2025-04-22 15:04:09,622 [src.satellite_marl_env] [DEBUG] --- Environment Reset Finished. Active agents: ['servicer', 'target'] ---
2025-04-22 15:04:09,623 [src.satellite_marl_env] [DEBUG] --- Step 0 Called (Active Agents: ['servicer', 'target']) ---
2025-04-22 15:04:09,624 [src.satellite_marl_env] [DEBUG] Step 0: Actions to apply: {'servicer': array([ 0.4156978 ,  0.68953544, -0.36937466, -0.5885888 ,  0.23635162,
        0.99963486], dtype=float32), 'target': array([ 0.29707238,  0.12079681, -0.2675369 ,  0.81722564, -0.4584231 ,
       -0.3264498 ], dtype=float32)}
2025-04-22 15:04:09,625 [src.satellite_marl_env] [DEBUG] Step 1 (Post mj_step): Servicer Pos=[ 1.09270325e-06  4.44840767e-07 -9.83723599e-07], Vel=[ 1.09270325e-04  4.44840767e-05 -9.83723599e-05]; Target Pos=[2.  0.5 0. ]; Contacts=0
2025-04-22 15:04:09,625 [src.satellite_marl_env] [DEBUG] CalcRewards Step 1: DockingDist=2.1471, RelVelMag=0.0002, IsDocked=False
2025-04-22 15:04:09,625 [src.satellite_marl_env] [DEBUG] CalcRewards Step 1: Episode is Over: False (Term=False, Trunc=False)
2025-04-22 15:04:09,625 [src.satellite_marl_env] [DEBUG] CalcRewards Step 1: Applying shaping rewards.
2025-04-22 15:04:09,625 [src.satellite_marl_env] [DEBUG] CalcRewards Step 1: PrevDist=2.1471, CurrDist=2.1471, Delta=0.0000, DeltaReward=0.0000
2025-04-22 15:04:09,625 [src.satellite_marl_env] [DEBUG] CalcRewards Step 1: DistPenalty=-0.1074
2025-04-22 15:04:09,625 [src.satellite_marl_env] [DEBUG] CalcRewards Step 1: VelPenalty=-0.0000
2025-04-22 15:04:09,625 [src.satellite_marl_env] [DEBUG] CalcRewards Step 1: ActionCostServ=-0.0015, ActionCostTarg=-0.0011
2025-04-22 15:04:09,626 [src.satellite_marl_env] [DEBUG] CalcRewards Step 1: Final Reward Components: {'servicer': {'base': 0.0, 'dist_delta': 0.0, 'dist_penalty': -0.10735455276791946, 'vel_penalty': -3.0721951888447585e-07, 'action_cost': -0.0014786040782928467, 'final': -0.10883346406573119}, 'target': {'base': 0.0, 'action_cost': -0.0010765681266784667, 'final': -0.0010765681266784667}}
2025-04-22 15:04:09,626 [src.satellite_marl_env] [DEBUG] CalcRewards Step 1: Updated prev_docking_distance = 2.1471
2025-04-22 15:04:09,626 [src.satellite_marl_env] [DEBUG] Step 1: Returning Rewards: {'servicer': -0.10883346406573119, 'target': -0.0010765681266784667}
2025-04-22 15:04:09,626 [src.satellite_marl_env] [DEBUG] Step 1: Returning Terminations: {'servicer': False, 'target': False}
2025-04-22 15:04:09,626 [src.satellite_marl_env] [DEBUG] Step 1: Returning Truncations: {'servicer': False, 'target': False}
2025-04-22 15:04:09,626 [src.satellite_marl_env] [DEBUG] --- Step 0 Finished. Duration: 0.0033s ---
2025-04-22 15:04:15,246 [__main__] [DEBUG] Creating RllibSatelliteEnv with config: {}
2025-04-22 15:04:15,249 [src.satellite_marl_env] [INFO] MuJoCo model loaded successfully from /Users/leo/RL-Spacecraft-Docking/src/xml_references/satellites.xml
2025-04-22 15:04:15,250 [src.satellite_marl_env] [DEBUG] MuJoCo IDs retrieved successfully.
2025-04-22 15:04:15,250 [src.satellite_marl_env] [DEBUG] --- Environment Reset Called ---
2025-04-22 15:04:15,250 [src.satellite_marl_env] [DEBUG] Resetting with seed: 42
2025-04-22 15:04:15,251 [src.satellite_marl_env] [DEBUG] MuJoCo data reset.
2025-04-22 15:04:15,251 [src.satellite_marl_env] [DEBUG] Reset: Initial Servicer qpos[0:7]: [0. 0. 0. 1. 0. 0. 0.]
2025-04-22 15:04:15,251 [src.satellite_marl_env] [DEBUG] Reset: Initial Target qpos[0:7]: [2.  0.5 0.  1.  0.  0.  0. ]
2025-04-22 15:04:15,252 [src.satellite_marl_env] [DEBUG] Reset: Initial Servicer qvel[0:6]: [0. 0. 0. 0. 0. 0.]
2025-04-22 15:04:15,252 [src.satellite_marl_env] [DEBUG] Reset: Initial Target qvel[0:6]: [0. 0. 0. 0. 0. 0.]
2025-04-22 15:04:15,252 [src.satellite_marl_env] [DEBUG] Initial mj_forward() completed.
2025-04-22 15:04:15,252 [src.satellite_marl_env] [DEBUG] Reset: Initial Obs 'servicer' (finite: True): [2.  0.5 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0. ]
2025-04-22 15:04:15,253 [src.satellite_marl_env] [DEBUG] Reset: Initial Obs 'target' (finite: True): [-2.  -0.5 -0.  -0.  -0.  -0.   1.   0.   0.   0.   0.   0.   0. ]
2025-04-22 15:04:15,253 [src.satellite_marl_env] [DEBUG] Reset: Initial Docking Dist: 2.1471, Rel Vel Mag: 0.0000
2025-04-22 15:04:15,253 [src.satellite_marl_env] [DEBUG] Reset: Initial Collision Detected: False (ncon=0)
2025-04-22 15:04:15,253 [src.satellite_marl_env] [DEBUG] Reset: Initialized prev_docking_distance = 2.1471
2025-04-22 15:04:15,253 [src.satellite_marl_env] [DEBUG] --- Environment Reset Finished. Active agents: ['servicer', 'target'] ---
2025-04-22 15:04:15,254 [src.satellite_marl_env] [DEBUG] --- Step 0 Called (Active Agents: ['servicer', 'target']) ---
2025-04-22 15:04:15,255 [src.satellite_marl_env] [DEBUG] Step 0: Actions to apply: {'servicer': array([-0.12529513, -0.09919789, -0.1387846 , -0.21011885,  0.8762909 ,
       -0.59899807], dtype=float32), 'target': array([-0.9555181 , -0.5237814 ,  0.8860001 ,  0.2540599 ,  0.5625053 ,
       -0.90554583], dtype=float32)}
2025-04-22 15:04:15,255 [src.satellite_marl_env] [DEBUG] Step 1 (Post mj_step): Servicer Pos=[-3.51387060e-06 -1.92571553e-06  3.25779052e-06], Vel=[-0.00035139 -0.00019257  0.00032578]; Target Pos=[2.  0.5 0. ]; Contacts=0
2025-04-22 15:04:15,255 [src.satellite_marl_env] [DEBUG] CalcRewards Step 1: DockingDist=2.1471, RelVelMag=0.0005, IsDocked=False
2025-04-22 15:04:15,256 [src.satellite_marl_env] [DEBUG] CalcRewards Step 1: Episode is Over: False (Term=False, Trunc=False)
2025-04-22 15:04:15,256 [src.satellite_marl_env] [DEBUG] CalcRewards Step 1: Applying shaping rewards.
2025-04-22 15:04:15,256 [src.satellite_marl_env] [DEBUG] CalcRewards Step 1: PrevDist=2.1471, CurrDist=2.1471, Delta=0.0000, DeltaReward=0.0000
2025-04-22 15:04:15,257 [src.satellite_marl_env] [DEBUG] CalcRewards Step 1: DistPenalty=-0.1074
2025-04-22 15:04:15,257 [src.satellite_marl_env] [DEBUG] CalcRewards Step 1: VelPenalty=-0.0000
2025-04-22 15:04:15,257 [src.satellite_marl_env] [DEBUG] CalcRewards Step 1: ActionCostServ=-0.0011, ActionCostTarg=-0.0018
2025-04-22 15:04:15,257 [src.satellite_marl_env] [DEBUG] CalcRewards Step 1: Final Reward Components: {'servicer': {'base': 0.0, 'dist_delta': 0.0, 'dist_penalty': -0.10735455276791946, 'vel_penalty': -1.0328381470776586e-06, 'action_cost': -0.0011025582551956177, 'final': -0.10845814386126215}, 'target': {'base': 0.0, 'action_cost': -0.0017813842296600343, 'final': -0.0017813842296600343}}
2025-04-22 15:04:15,257 [src.satellite_marl_env] [DEBUG] CalcRewards Step 1: Updated prev_docking_distance = 2.1471
2025-04-22 15:04:15,258 [src.satellite_marl_env] [DEBUG] Step 1: Returning Rewards: {'servicer': -0.10845814386126215, 'target': -0.0017813842296600343}
2025-04-22 15:04:15,258 [src.satellite_marl_env] [DEBUG] Step 1: Returning Terminations: {'servicer': False, 'target': False}
2025-04-22 15:04:15,258 [src.satellite_marl_env] [DEBUG] Step 1: Returning Truncations: {'servicer': False, 'target': False}
2025-04-22 15:04:15,258 [src.satellite_marl_env] [DEBUG] --- Step 0 Finished. Duration: 0.0041s ---
2025-04-22 15:04:16,782 [__main__] [INFO] Algorithm built successfully.
2025-04-22 15:04:16,783 [__main__] [WARNING] Could not retrieve policy/module class name: 'PPOConfig' object has no attribute 'uses_new_api_stack'
2025-04-22 15:04:16,783 [__main__] [INFO] 
--- Starting Training for 100 iterations ---
2025-04-22 15:04:16,783 [__main__] [DEBUG] --- Starting Training Iteration 1/100 ---
2025-04-22 15:04:21,592 [__main__] [DEBUG] --- Finished Training Iteration 1/100 ---
2025-04-22 15:04:21,593 [__main__] [INFO] Iter: 1/100, Ts(iter): 0, Ts(total): 2800, Reward (None): nan, Loss(serv): nan, Loss(targ): nan, Time: 4.81s
2025-04-22 15:04:21,602 [__main__] [DEBUG] Full result dict at iter 1:
{ 'config': { '_disable_action_flattening': False,
              '_disable_execution_plan_api': -1,
              '_disable_initialize_loss_from_dummy_batch': False,
              '_disable_preprocessor_api': False,
              '_dont_auto_sync_env_runner_states': False,
              '_enable_rl_module_api': -1,
              '_env_to_module_connector': None,
              '_fake_gpus': False,
              '_is_atari': None,
              '_learner_class': None,
              '_learner_connector': None,
              '_model_config': {},
              '_module_to_env_connector': None,
              '_per_module_overrides': {},
              '_prior_exploration_config': {'type': 'StochasticSampling'},
              '_rl_module_spec': None,
              '_tf_policy_handles_more_than_one_loss': False,
              '_torch_grad_scaler_class': None,
              '_torch_lr_scheduler_classes': None,
              '_train_batch_size_per_learner': None,
              '_use_msgpack_checkpoints': False,
              '_validate_config': True,
              'action_mask_key': 'action_mask',
              'action_space': None,
              'actions_in_input_normalized': False,
              'add_default_connectors_to_env_to_module_pipeline': True,
              'add_default_connectors_to_learner_pipeline': True,
              'add_default_connectors_to_module_to_env_pipeline': True,
              'algorithm_config_overrides_per_module': {},
              'always_attach_evaluation_results': -1,
              'auto_wrap_old_gym_envs': -1,
              'batch_mode': 'truncate_episodes',
              'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>,
              'callbacks_on_algorithm_init': None,
              'callbacks_on_checkpoint_loaded': None,
              'callbacks_on_env_runners_recreated': None,
              'callbacks_on_environment_created': None,
              'callbacks_on_episode_created': None,
              'callbacks_on_episode_end': None,
              'callbacks_on_episode_start': None,
              'callbacks_on_episode_step': None,
              'callbacks_on_evaluate_end': None,
              'callbacks_on_evaluate_start': None,
              'callbacks_on_sample_end': None,
              'callbacks_on_train_result': None,
              'checkpoint_trainable_policies_only': False,
              'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>,
              'clip_actions': False,
              'clip_param': 0.2,
              'clip_rewards': None,
              'compress_observations': False,
              'count_steps_by': 'env_steps',
              'create_env_on_driver': False,
              'custom_async_evaluation_function': -1,
              'custom_eval_function': None,
              'custom_resources_per_env_runner': {},
              'dataset_num_iters_per_learner': None,
              'delay_between_env_runner_restarts_s': 60.0,
              'disable_env_checking': False,
              'eager_max_retraces': 20,
              'eager_tracing': True,
              'enable_async_evaluation': -1,
              'enable_connectors': -1,
              'enable_env_runner_and_connector_v2': True,
              'enable_rl_module_and_learner': True,
              'enable_tf1_exec_eagerly': False,
              'entropy_coeff': 2e-06,
              'entropy_coeff_schedule': None,
              'env': 'satellite_marl',
              'env_config': {},
              'env_runner_cls': None,
              'env_runner_health_probe_timeout_s': 30.0,
              'env_runner_restore_timeout_s': 1800.0,
              'env_task_fn': -1,
              'episode_lookback_horizon': 1,
              'episodes_to_numpy': True,
              'evaluation_config': { 'explore': False,
                                     'num_cpus_per_env_runner': 1,
                                     'observation_filter': 'MeanStdFilter'},
              'evaluation_duration': 5,
              'evaluation_duration_unit': 'episodes',
              'evaluation_force_reset_envs_before_iteration': True,
              'evaluation_interval': 10,
              'evaluation_num_env_runners': 1,
              'evaluation_parallel_to_training': True,
              'evaluation_sample_timeout_s': 120.0,
              'exploration_config': {},
              'explore': True,
              'export_native_model_files': False,
              'extra_python_environs_for_driver': {},
              'extra_python_environs_for_worker': {},
              'fake_sampler': False,
              'framework': 'torch',
              'gamma': 0.96,
              'grad_clip': 0.5,
              'grad_clip_by': 'global_norm',
              'gym_env_vectorize_mode': 'SYNC',
              'ignore_env_runner_failures': False,
              'in_evaluation': False,
              'input': 'sampler',
              'input_compress_columns': ['obs', 'new_obs'],
              'input_config': {},
              'input_filesystem': None,
              'input_filesystem_kwargs': {},
              'input_read_batch_size': None,
              'input_read_episodes': False,
              'input_read_method': 'read_parquet',
              'input_read_method_kwargs': {},
              'input_read_sample_batches': False,
              'input_read_schema': {},
              'input_spaces_jsonable': True,
              'iter_batches_kwargs': {},
              'keep_per_episode_custom_metrics': False,
              'kl_coeff': 0.2,
              'kl_target': 0.01,
              'lambda': 1.0,
              'learner_config_dict': {},
              'local_gpu_idx': 0,
              'local_tf_session_args': {'inter_op_parallelism_threads': 8, 'intra_op_parallelism_threads': 8},
              'log_gradients': True,
              'log_level': 'INFO',
              'log_sys_usage': True,
              'logger_config': None,
              'logger_creator': None,
              'lr': 0.0003,
              'lr_schedule': None,
              'map_batches_kwargs': {},
              'materialize_data': False,
              'materialize_mapped_data': True,
              'max_num_env_runner_restarts': 1000,
              'max_requests_in_flight_per_aggregator_actor': 3,
              'max_requests_in_flight_per_env_runner': 1,
              'max_requests_in_flight_per_learner': 3,
              'metrics_episode_collection_timeout_s': 60.0,
              'metrics_num_episodes_for_smoothing': 100,
              'min_sample_timesteps_per_iteration': 0,
              'min_time_s_per_iteration': None,
              'min_train_timesteps_per_iteration': 0,
              'minibatch_size': 128,
              'model': { '_disable_action_flattening': False,
                         '_disable_preprocessor_api': False,
                         '_time_major': False,
                         '_use_default_native_models': -1,
                         'always_check_shapes': False,
                         'attention_dim': 64,
                         'attention_head_dim': 32,
                         'attention_init_gru_gate_bias': 2.0,
                         'attention_memory_inference': 50,
                         'attention_memory_training': 50,
                         'attention_num_heads': 1,
                         'attention_num_transformer_units': 1,
                         'attention_position_wise_mlp_dim': 32,
                         'attention_use_n_prev_actions': 0,
                         'attention_use_n_prev_rewards': 0,
                         'conv_activation': 'relu',
                         'conv_bias_initializer': None,
                         'conv_bias_initializer_config': None,
                         'conv_filters': None,
                         'conv_kernel_initializer': None,
                         'conv_kernel_initializer_config': None,
                         'conv_transpose_bias_initializer': None,
                         'conv_transpose_bias_initializer_config': None,
                         'conv_transpose_kernel_initializer': None,
                         'conv_transpose_kernel_initializer_config': None,
                         'custom_action_dist': None,
                         'custom_model': None,
                         'custom_model_config': {},
                         'custom_preprocessor': None,
                         'dim': 84,
                         'encoder_latent_dim': None,
                         'fcnet_activation': 'tanh',
                         'fcnet_bias_initializer': None,
                         'fcnet_bias_initializer_config': None,
                         'fcnet_hiddens': [256, 256],
                         'fcnet_weights_initializer': None,
                         'fcnet_weights_initializer_config': None,
                         'framestack': True,
                         'free_log_std': False,
                         'grayscale': False,
                         'log_std_clip_param': 20.0,
                         'lstm_bias_initializer': None,
                         'lstm_bias_initializer_config': None,
                         'lstm_cell_size': 256,
                         'lstm_use_prev_action': False,
                         'lstm_use_prev_action_reward': -1,
                         'lstm_use_prev_reward': False,
                         'lstm_weights_initializer': None,
                         'lstm_weights_initializer_config': None,
                         'max_seq_len': 20,
                         'no_final_linear': False,
                         'post_fcnet_activation': 'relu',
                         'post_fcnet_bias_initializer': None,
                         'post_fcnet_bias_initializer_config': None,
                         'post_fcnet_hiddens': [],
                         'post_fcnet_weights_initializer': None,
                         'post_fcnet_weights_initializer_config': None,
                         'use_attention': False,
                         'use_lstm': False,
                         'vf_share_layers': False,
                         'zero_mean': True},
              'normalize_actions': True,
              'num_aggregator_actors_per_learner': 0,
              'num_consecutive_env_runner_failures_tolerance': 100,
              'num_cpus_for_main_process': 1,
              'num_cpus_per_env_runner': 1,
              'num_cpus_per_learner': 'auto',
              'num_env_runners': 14,
              'num_envs_per_env_runner': 1,
              'num_epochs': 4,
              'num_gpus': 0,
              'num_gpus_per_env_runner': 0,
              'num_gpus_per_learner': 0,
              'num_learners': 0,
              'observation_filter': 'MeanStdFilter',
              'observation_fn': None,
              'observation_space': None,
              'off_policy_estimation_methods': {},
              'offline_data_class': None,
              'offline_sampling': False,
              'ope_split_batch_by_episode': True,
              'optimizer': {},
              'output': None,
              'output_compress_columns': ['obs', 'new_obs'],
              'output_config': {},
              'output_filesystem': None,
              'output_filesystem_kwargs': {},
              'output_max_file_size': 67108864,
              'output_max_rows_per_file': None,
              'output_write_episodes': True,
              'output_write_method': 'write_parquet',
              'output_write_method_kwargs': {},
              'output_write_remaining_data': False,
              'placement_strategy': 'PACK',
              'policies': { 'servicer': (None, Box(-inf, inf, (13,), float32), Box(-1.0, 1.0, (6,), float32), None),
                            'target': (None, Box(-inf, inf, (13,), float32), Box(-1.0, 1.0, (6,), float32), None)},
              'policies_to_train': None,
              'policy_map_cache': -1,
              'policy_map_capacity': 100,
              'policy_mapping_fn': <function <lambda> at 0x15ff191c0>,
              'policy_states_are_swappable': False,
              'postprocess_inputs': False,
              'prelearner_buffer_class': None,
              'prelearner_buffer_kwargs': {},
              'prelearner_class': None,
              'prelearner_module_synch_period': 10,
              'preprocessor_pref': 'deepmind',
              'remote_env_batch_wait_ms': 0,
              'remote_worker_envs': False,
              'render_env': False,
              'replay_sequence_length': None,
              'restart_failed_env_runners': True,
              'restart_failed_sub_environments': False,
              'rollout_fragment_length': 200,
              'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,
              'sample_timeout_s': 60.0,
              'sampler_perf_stats_ema_coef': None,
              'seed': 42,
              'sgd_minibatch_size': -1,
              'shuffle_batch_per_epoch': True,
              'shuffle_buffer_size': 0,
              'simple_optimizer': True,
              'sync_filters_on_rollout_workers_timeout_s': 10.0,
              'synchronize_filters': -1,
              'tf_session_args': { 'allow_soft_placement': True,
                                   'device_count': {'CPU': 1},
                                   'gpu_options': {'allow_growth': True},
                                   'inter_op_parallelism_threads': 2,
                                   'intra_op_parallelism_threads': 2,
                                   'log_device_placement': False},
              'torch_compile_learner': False,
              'torch_compile_learner_dynamo_backend': 'aot_eager',
              'torch_compile_learner_dynamo_mode': None,
              'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,
              'torch_compile_worker': False,
              'torch_compile_worker_dynamo_backend': 'aot_eager',
              'torch_compile_worker_dynamo_mode': None,
              'torch_ddp_kwargs': {},
              'torch_skip_nan_gradients': False,
              'train_batch_size': 2800,
              'update_worker_filter_stats': True,
              'use_critic': True,
              'use_gae': True,
              'use_kl_loss': True,
              'use_worker_filter_stats': True,
              'validate_env_runners_after_construction': True,
              'vf_clip_param': 10.0,
              'vf_loss_coeff': 1.0,
              'vf_share_layers': -1,
              'worker_cls': -1},
  'date': '2025-04-22_15-04-21',
  'done': False,
  'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0},
  'env_runners': { 'connector_pipeline_timer': 0.0011970073558456665,
                   'env_reset_timer': 0.00286987836755413,
                   'env_step_timer': 0.003461642405621014,
                   'env_to_module_connector': { 'connector_pipeline_timer': 0.000940859757554516,
                                                'timers': { 'connectors': { 'add_observations_from_episodes_to_batch': 0.0001080596785137003,
                                                                            'add_states_from_episodes_to_batch': 3.537806448591163e-05,
                                                                            'add_time_dim_to_batch_and_zero_pad': 6.802780696217146e-05,
                                                                            'agent_to_module_mapping': 2.5223120533007846e-05,
                                                                            'batch_individual_items': 0.00014811475507824364,
                                                                            'numpy_to_tensor': 0.000258125824344479}}},
                   'env_to_module_sum_episodes_length_in': 114.26398781093823,
                   'env_to_module_sum_episodes_length_out': 114.26398781093823,
                   'module_to_env_connector': { 'connector_pipeline_timer': 0.0027712425001666717,
                                                'timers': { 'connectors': { 'get_actions': 0.0012480974112996134,
                                                                            'listify_data_for_vector_env': 6.892105632592628e-05,
                                                                            'module_to_agent_unmapping': 2.3660255105508113e-05,
                                                                            'normalize_and_clip_actions': 0.0004639263995288701,
                                                                            'remove_single_ts_time_rank_from_batch': 7.533638172922632e-06,
                                                                            'tensor_to_numpy': 0.0003982147775710587,
                                                                            'un_batch_to_individual_items': 0.00016258563995446917}}},
                   'num_agent_steps_sampled': {'servicer': 2800, 'target': 2800},
                   'num_agent_steps_sampled_lifetime': {'servicer': 2800, 'target': 2800},
                   'num_env_steps_sampled': 2800,
                   'num_env_steps_sampled_lifetime': 2800,
                   'num_env_steps_sampled_lifetime_throughput': nan,
                   'num_module_steps_sampled': {'servicer': 2800, 'target': 2800},
                   'num_module_steps_sampled_lifetime': {'servicer': 2800, 'target': 2800},
                   'rlmodule_inference_timer': 0.0013804338681089046,
                   'sample': 1.90542147357233,
                   'timers': { 'connectors': { 'add_observations_from_episodes_to_batch': 0.00013324770926763968,
                                               'add_states_from_episodes_to_batch': 4.1368920522342834e-05,
                                               'add_time_dim_to_batch_and_zero_pad': 5.5244358788643565e-05,
                                               'agent_to_module_mapping': 2.9257132804819514e-05,
                                               'batch_individual_items': 0.00015549228243928934,
                                               'numpy_to_tensor': 0.00021250315226747522}},
                   'weights_seq_no': 0.0},
  'fault_tolerance': {'num_healthy_workers': 14, 'num_remote_worker_restarts': 0},
  'hostname': 'Nathans-MacBook-Pro.local',
  'iterations_since_restore': 1,
  'learners': { '__all_modules__': { 'learner_connector': { 'connector_pipeline_timer': 0.6799244689755142,
                                                            'timers': { 'connectors': { 'add_columns_from_episodes_to_train_batch': 0.18588133598677814,
                                                                                        'add_observations_from_episodes_to_batch': 0.0005848039872944355,
                                                                                        'add_one_ts_to_episodes_and_truncate': 0.011984982993453741,
                                                                                        'add_states_from_episodes_to_batch': 2.0683975890278816e-05,
                                                                                        'add_time_dim_to_batch_and_zero_pad': 4.589097807183862e-05,
                                                                                        'agent_to_module_mapping': 0.009615813032723963,
                                                                                        'batch_individual_items': 0.37361906800651923,
                                                                                        'general_advantage_estimation': 0.09721036598784849,
                                                                                        'numpy_to_tensor': 0.00036340299993753433}}},
                                     'learner_connector_sum_episodes_length_in': 2800,
                                     'learner_connector_sum_episodes_length_out': 2800,
                                     'num_env_steps_trained': 246400,
                                     'num_env_steps_trained_lifetime': 246400,
                                     'num_env_steps_trained_lifetime_throughput': nan,
                                     'num_module_steps_trained': 22528,
                                     'num_module_steps_trained_lifetime': 22528,
                                     'num_non_trainable_parameters': 0,
                                     'num_trainable_parameters': 284186},
                'servicer': { 'curr_entropy_coeff': 2e-06,
                              'curr_kl_coeff': 0.20000000298023224,
                              'default_optimizer_learning_rate': 0.0003,
                              'diff_num_grad_updates_vs_sampler_policy': 1.0,
                              'entropy': 8.052194595336914,
                              'gradients_default_optimizer_global_norm': 1.4289926290512085,
                              'mean_kl_loss': 0.009336228482425213,
                              'module_train_batch_size_mean': 128.0,
                              'num_module_steps_trained': 11264,
                              'num_module_steps_trained_lifetime': 11264,
                              'num_trainable_parameters': 142093,
                              'policy_loss': 0.04937974363565445,
                              'total_loss': 0.30525821447372437,
                              'vf_explained_var': -0.0011887550354003906,
                              'vf_loss': 0.2540273368358612,
                              'vf_loss_unclipped': 0.2540273368358612,
                              'weights_seq_no': 1.0},
                'target': { 'curr_entropy_coeff': 2e-06,
                            'curr_kl_coeff': 0.20000000298023224,
                            'default_optimizer_learning_rate': 0.0003,
                            'diff_num_grad_updates_vs_sampler_policy': 1.0,
                            'entropy': 8.65345287322998,
                            'gradients_default_optimizer_global_norm': 1.1237666606903076,
                            'mean_kl_loss': 0.010896939784288406,
                            'module_train_batch_size_mean': 128.0,
                            'num_module_steps_trained': 11264,
                            'num_module_steps_trained_lifetime': 11264,
                            'num_trainable_parameters': 142093,
                            'policy_loss': 0.1332482546567917,
                            'total_loss': 0.13549765944480896,
                            'vf_explained_var': 0.002803623676300049,
                            'vf_loss': 8.730831177672371e-05,
                            'vf_loss_unclipped': 8.730831177672371e-05,
                            'weights_seq_no': 1.0}},
  'node_ip': '127.0.0.1',
  'num_env_steps_sampled_lifetime': 2800,
  'num_env_steps_sampled_lifetime_throughput': nan,
  'num_training_step_calls_per_iteration': 1,
  'perf': {'cpu_util_percent': 65.18571428571428, 'ram_util_percent': 65.18571428571428},
  'pid': 79425,
  'time_since_restore': 4.77166485786438,
  'time_this_iter_s': 4.77166485786438,
  'time_total_s': 4.77166485786438,
  'timers': { 'env_runner_sampling_timer': 1.9676823689951561,
              'learner_update_timer': 2.7663746179896407,
              'restore_env_runners': 3.58769902959466e-05,
              'synch_weights': 0.011033112998120487,
              'training_iteration': 4.747890317987185,
              'training_step': 4.747508453961927},
  'timestamp': 1745348661,
  'training_iteration': 1,
  'trial_id': 'default'}
2025-04-22 15:04:21,605 [__main__] [DEBUG] --- Starting Training Iteration 2/100 ---
2025-04-22 15:04:25,746 [__main__] [DEBUG] --- Finished Training Iteration 2/100 ---
2025-04-22 15:04:25,747 [__main__] [INFO] Iter: 2/100, Ts(iter): 0, Ts(total): 5600, Reward (None): nan, Loss(serv): nan, Loss(targ): nan, Time: 4.14s
2025-04-22 15:04:25,756 [__main__] [DEBUG] Full result dict at iter 2:
{ 'config': { '_disable_action_flattening': False,
              '_disable_execution_plan_api': -1,
              '_disable_initialize_loss_from_dummy_batch': False,
              '_disable_preprocessor_api': False,
              '_dont_auto_sync_env_runner_states': False,
              '_enable_rl_module_api': -1,
              '_env_to_module_connector': None,
              '_fake_gpus': False,
              '_is_atari': None,
              '_learner_class': None,
              '_learner_connector': None,
              '_model_config': {},
              '_module_to_env_connector': None,
              '_per_module_overrides': {},
              '_prior_exploration_config': {'type': 'StochasticSampling'},
              '_rl_module_spec': None,
              '_tf_policy_handles_more_than_one_loss': False,
              '_torch_grad_scaler_class': None,
              '_torch_lr_scheduler_classes': None,
              '_train_batch_size_per_learner': None,
              '_use_msgpack_checkpoints': False,
              '_validate_config': True,
              'action_mask_key': 'action_mask',
              'action_space': None,
              'actions_in_input_normalized': False,
              'add_default_connectors_to_env_to_module_pipeline': True,
              'add_default_connectors_to_learner_pipeline': True,
              'add_default_connectors_to_module_to_env_pipeline': True,
              'algorithm_config_overrides_per_module': {},
              'always_attach_evaluation_results': -1,
              'auto_wrap_old_gym_envs': -1,
              'batch_mode': 'truncate_episodes',
              'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>,
              'callbacks_on_algorithm_init': None,
              'callbacks_on_checkpoint_loaded': None,
              'callbacks_on_env_runners_recreated': None,
              'callbacks_on_environment_created': None,
              'callbacks_on_episode_created': None,
              'callbacks_on_episode_end': None,
              'callbacks_on_episode_start': None,
              'callbacks_on_episode_step': None,
              'callbacks_on_evaluate_end': None,
              'callbacks_on_evaluate_start': None,
              'callbacks_on_sample_end': None,
              'callbacks_on_train_result': None,
              'checkpoint_trainable_policies_only': False,
              'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>,
              'clip_actions': False,
              'clip_param': 0.2,
              'clip_rewards': None,
              'compress_observations': False,
              'count_steps_by': 'env_steps',
              'create_env_on_driver': False,
              'custom_async_evaluation_function': -1,
              'custom_eval_function': None,
              'custom_resources_per_env_runner': {},
              'dataset_num_iters_per_learner': None,
              'delay_between_env_runner_restarts_s': 60.0,
              'disable_env_checking': False,
              'eager_max_retraces': 20,
              'eager_tracing': True,
              'enable_async_evaluation': -1,
              'enable_connectors': -1,
              'enable_env_runner_and_connector_v2': True,
              'enable_rl_module_and_learner': True,
              'enable_tf1_exec_eagerly': False,
              'entropy_coeff': 2e-06,
              'entropy_coeff_schedule': None,
              'env': 'satellite_marl',
              'env_config': {},
              'env_runner_cls': None,
              'env_runner_health_probe_timeout_s': 30.0,
              'env_runner_restore_timeout_s': 1800.0,
              'env_task_fn': -1,
              'episode_lookback_horizon': 1,
              'episodes_to_numpy': True,
              'evaluation_config': { 'explore': False,
                                     'num_cpus_per_env_runner': 1,
                                     'observation_filter': 'MeanStdFilter'},
              'evaluation_duration': 5,
              'evaluation_duration_unit': 'episodes',
              'evaluation_force_reset_envs_before_iteration': True,
              'evaluation_interval': 10,
              'evaluation_num_env_runners': 1,
              'evaluation_parallel_to_training': True,
              'evaluation_sample_timeout_s': 120.0,
              'exploration_config': {},
              'explore': True,
              'export_native_model_files': False,
              'extra_python_environs_for_driver': {},
              'extra_python_environs_for_worker': {},
              'fake_sampler': False,
              'framework': 'torch',
              'gamma': 0.96,
              'grad_clip': 0.5,
              'grad_clip_by': 'global_norm',
              'gym_env_vectorize_mode': 'SYNC',
              'ignore_env_runner_failures': False,
              'in_evaluation': False,
              'input': 'sampler',
              'input_compress_columns': ['obs', 'new_obs'],
              'input_config': {},
              'input_filesystem': None,
              'input_filesystem_kwargs': {},
              'input_read_batch_size': None,
              'input_read_episodes': False,
              'input_read_method': 'read_parquet',
              'input_read_method_kwargs': {},
              'input_read_sample_batches': False,
              'input_read_schema': {},
              'input_spaces_jsonable': True,
              'iter_batches_kwargs': {},
              'keep_per_episode_custom_metrics': False,
              'kl_coeff': 0.2,
              'kl_target': 0.01,
              'lambda': 1.0,
              'learner_config_dict': {},
              'local_gpu_idx': 0,
              'local_tf_session_args': {'inter_op_parallelism_threads': 8, 'intra_op_parallelism_threads': 8},
              'log_gradients': True,
              'log_level': 'INFO',
              'log_sys_usage': True,
              'logger_config': None,
              'logger_creator': None,
              'lr': 0.0003,
              'lr_schedule': None,
              'map_batches_kwargs': {},
              'materialize_data': False,
              'materialize_mapped_data': True,
              'max_num_env_runner_restarts': 1000,
              'max_requests_in_flight_per_aggregator_actor': 3,
              'max_requests_in_flight_per_env_runner': 1,
              'max_requests_in_flight_per_learner': 3,
              'metrics_episode_collection_timeout_s': 60.0,
              'metrics_num_episodes_for_smoothing': 100,
              'min_sample_timesteps_per_iteration': 0,
              'min_time_s_per_iteration': None,
              'min_train_timesteps_per_iteration': 0,
              'minibatch_size': 128,
              'model': { '_disable_action_flattening': False,
                         '_disable_preprocessor_api': False,
                         '_time_major': False,
                         '_use_default_native_models': -1,
                         'always_check_shapes': False,
                         'attention_dim': 64,
                         'attention_head_dim': 32,
                         'attention_init_gru_gate_bias': 2.0,
                         'attention_memory_inference': 50,
                         'attention_memory_training': 50,
                         'attention_num_heads': 1,
                         'attention_num_transformer_units': 1,
                         'attention_position_wise_mlp_dim': 32,
                         'attention_use_n_prev_actions': 0,
                         'attention_use_n_prev_rewards': 0,
                         'conv_activation': 'relu',
                         'conv_bias_initializer': None,
                         'conv_bias_initializer_config': None,
                         'conv_filters': None,
                         'conv_kernel_initializer': None,
                         'conv_kernel_initializer_config': None,
                         'conv_transpose_bias_initializer': None,
                         'conv_transpose_bias_initializer_config': None,
                         'conv_transpose_kernel_initializer': None,
                         'conv_transpose_kernel_initializer_config': None,
                         'custom_action_dist': None,
                         'custom_model': None,
                         'custom_model_config': {},
                         'custom_preprocessor': None,
                         'dim': 84,
                         'encoder_latent_dim': None,
                         'fcnet_activation': 'tanh',
                         'fcnet_bias_initializer': None,
                         'fcnet_bias_initializer_config': None,
                         'fcnet_hiddens': [256, 256],
                         'fcnet_weights_initializer': None,
                         'fcnet_weights_initializer_config': None,
                         'framestack': True,
                         'free_log_std': False,
                         'grayscale': False,
                         'log_std_clip_param': 20.0,
                         'lstm_bias_initializer': None,
                         'lstm_bias_initializer_config': None,
                         'lstm_cell_size': 256,
                         'lstm_use_prev_action': False,
                         'lstm_use_prev_action_reward': -1,
                         'lstm_use_prev_reward': False,
                         'lstm_weights_initializer': None,
                         'lstm_weights_initializer_config': None,
                         'max_seq_len': 20,
                         'no_final_linear': False,
                         'post_fcnet_activation': 'relu',
                         'post_fcnet_bias_initializer': None,
                         'post_fcnet_bias_initializer_config': None,
                         'post_fcnet_hiddens': [],
                         'post_fcnet_weights_initializer': None,
                         'post_fcnet_weights_initializer_config': None,
                         'use_attention': False,
                         'use_lstm': False,
                         'vf_share_layers': False,
                         'zero_mean': True},
              'normalize_actions': True,
              'num_aggregator_actors_per_learner': 0,
              'num_consecutive_env_runner_failures_tolerance': 100,
              'num_cpus_for_main_process': 1,
              'num_cpus_per_env_runner': 1,
              'num_cpus_per_learner': 'auto',
              'num_env_runners': 14,
              'num_envs_per_env_runner': 1,
              'num_epochs': 4,
              'num_gpus': 0,
              'num_gpus_per_env_runner': 0,
              'num_gpus_per_learner': 0,
              'num_learners': 0,
              'observation_filter': 'MeanStdFilter',
              'observation_fn': None,
              'observation_space': None,
              'off_policy_estimation_methods': {},
              'offline_data_class': None,
              'offline_sampling': False,
              'ope_split_batch_by_episode': True,
              'optimizer': {},
              'output': None,
              'output_compress_columns': ['obs', 'new_obs'],
              'output_config': {},
              'output_filesystem': None,
              'output_filesystem_kwargs': {},
              'output_max_file_size': 67108864,
              'output_max_rows_per_file': None,
              'output_write_episodes': True,
              'output_write_method': 'write_parquet',
              'output_write_method_kwargs': {},
              'output_write_remaining_data': False,
              'placement_strategy': 'PACK',
              'policies': { 'servicer': (None, Box(-inf, inf, (13,), float32), Box(-1.0, 1.0, (6,), float32), None),
                            'target': (None, Box(-inf, inf, (13,), float32), Box(-1.0, 1.0, (6,), float32), None)},
              'policies_to_train': None,
              'policy_map_cache': -1,
              'policy_map_capacity': 100,
              'policy_mapping_fn': <function <lambda> at 0x15ff191c0>,
              'policy_states_are_swappable': False,
              'postprocess_inputs': False,
              'prelearner_buffer_class': None,
              'prelearner_buffer_kwargs': {},
              'prelearner_class': None,
              'prelearner_module_synch_period': 10,
              'preprocessor_pref': 'deepmind',
              'remote_env_batch_wait_ms': 0,
              'remote_worker_envs': False,
              'render_env': False,
              'replay_sequence_length': None,
              'restart_failed_env_runners': True,
              'restart_failed_sub_environments': False,
              'rollout_fragment_length': 200,
              'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,
              'sample_timeout_s': 60.0,
              'sampler_perf_stats_ema_coef': None,
              'seed': 42,
              'sgd_minibatch_size': -1,
              'shuffle_batch_per_epoch': True,
              'shuffle_buffer_size': 0,
              'simple_optimizer': True,
              'sync_filters_on_rollout_workers_timeout_s': 10.0,
              'synchronize_filters': -1,
              'tf_session_args': { 'allow_soft_placement': True,
                                   'device_count': {'CPU': 1},
                                   'gpu_options': {'allow_growth': True},
                                   'inter_op_parallelism_threads': 2,
                                   'intra_op_parallelism_threads': 2,
                                   'log_device_placement': False},
              'torch_compile_learner': False,
              'torch_compile_learner_dynamo_backend': 'aot_eager',
              'torch_compile_learner_dynamo_mode': None,
              'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>,
              'torch_compile_worker': False,
              'torch_compile_worker_dynamo_backend': 'aot_eager',
              'torch_compile_worker_dynamo_mode': None,
              'torch_ddp_kwargs': {},
              'torch_skip_nan_gradients': False,
              'train_batch_size': 2800,
              'update_worker_filter_stats': True,
              'use_critic': True,
              'use_gae': True,
              'use_kl_loss': True,
              'use_worker_filter_stats': True,
              'validate_env_runners_after_construction': True,
              'vf_clip_param': 10.0,
              'vf_loss_coeff': 1.0,
              'vf_share_layers': -1,
              'worker_cls': -1},
  'date': '2025-04-22_15-04-25',
  'done': False,
  'env_runner_group': {'actor_manager_num_outstanding_async_reqs': 0},
  'env_runners': { 'connector_pipeline_timer': 0.0011970073558456665,
                   'env_reset_timer': 0.00286987836755413,
                   'env_step_timer': 0.0034285146594170593,
                   'env_to_module_connector': { 'connector_pipeline_timer': 0.000928206032189529,
                                                'timers': { 'connectors': { 'add_observations_from_episodes_to_batch': 0.00010669051525355457,
                                                                            'add_states_from_episodes_to_batch': 3.483944603371102e-05,
                                                                            'add_time_dim_to_batch_and_zero_pad': 6.645360402316502e-05,
                                                                            'agent_to_module_mapping': 2.465637441248744e-05,
                                                                            'batch_individual_items': 0.00014511079177598681,
                                                                            'numpy_to_tensor': 0.0002573719443023223}}},
                   'env_to_module_sum_episodes_length_in': 116.27336497992883,
                   'env_to_module_sum_episodes_length_out': 116.27336497992883,
                   'module_to_env_connector': { 'connector_pipeline_timer': 0.002725333171449302,
                                                'timers': { 'connectors': { 'get_actions': 0.0012208722999214135,
                                                                            'listify_data_for_vector_env': 6.623152017084694e-05,
                                                                            'module_to_agent_unmapping': 2.3431359173468016e-05,
                                                                            'normalize_and_clip_actions': 0.00045469758162299716,
                                                                            'remove_single_ts_time_rank_from_batch': 7.861884573233294e-06,
                                                                            'tensor_to_numpy': 0.00039335437095770336,
                                                                            'un_batch_to_individual_items': 0.0001612818429151639}}},
                   'num_agent_steps_sampled': {'servicer': 2800, 'target': 2800},
                   'num_agent_steps_sampled_lifetime': {'servicer': 5600, 'target': 5600},
                   'num_env_steps_sampled': 2800,
                   'num_env_steps_sampled_lifetime': 5600,
                   'num_env_steps_sampled_lifetime_throughput': 674.0713194016384,
                   'num_module_steps_sampled': {'servicer': 2800, 'target': 2800},
                   'num_module_steps_sampled_lifetime': {'servicer': 5600, 'target': 5600},
                   'rlmodule_inference_timer': 0.0013530885185447394,
                   'sample': 1.9052317716531741,
                   'time_between_sampling': 2.9172572394225944,
                   'timers': { 'connectors': { 'add_observations_from_episodes_to_batch': 0.00013324770926763968,
                                               'add_states_from_episodes_to_batch': 4.1368920522342834e-05,
                                               'add_time_dim_to_batch_and_zero_pad': 5.5244358788643565e-05,
                                               'agent_to_module_mapping': 2.9257132804819514e-05,
                                               'batch_individual_items': 0.00015549228243928934,
                                               'numpy_to_tensor': 0.00021250315226747522}},
                   'weights_seq_no': 1.0},
  'fault_tolerance': {'num_healthy_workers': 14, 'num_remote_worker_restarts': 0},
  'hostname': 'Nathans-MacBook-Pro.local',
  'iterations_since_restore': 2,
  'learners': { '__all_modules__': { 'learner_connector': { 'connector_pipeline_timer': 0.6798948324219207,
                                                            'timers': { 'connectors': { 'add_columns_from_episodes_to_train_batch': 0.1858807999507815,
                                                                                        'add_observations_from_episodes_to_batch': 0.0005847967926936689,
                                                                                        'add_one_ts_to_episodes_and_truncate': 0.011984989120357203,
                                                                                        'add_states_from_episodes_to_batch': 2.0683550095418468e-05,
                                                                                        'add_time_dim_to_batch_and_zero_pad': 4.589088376960717e-05,
                                                                                        'agent_to_module_mapping': 0.009615704655216542,
                                                                                        'batch_individual_items': 0.37359633131641895,
                                                                                        'general_advantage_estimation': 0.09720413484314921,
                                                                                        'numpy_to_tensor': 0.00036340445674140935}}},
                                     'learner_connector_sum_episodes_length_in': 2800.0,
                                     'learner_connector_sum_episodes_length_out': 2800.0,
                                     'num_env_steps_trained': 246400,
                                     'num_env_steps_trained_lifetime': 492800,
                                     'num_env_steps_trained_lifetime_throughput': 0.0,
                                     'num_module_steps_trained': 22528,
                                     'num_module_steps_trained_lifetime': 45056,
                                     'num_non_trainable_parameters': 0.0,
                                     'num_trainable_parameters': 284186.0},
                'servicer': { 'curr_entropy_coeff': 2e-06,
                              'curr_kl_coeff': 0.20000000298023224,
                              'default_optimizer_learning_rate': 0.0003,
                              'diff_num_grad_updates_vs_sampler_policy': 1.0,
                              'entropy': 8.149285316467285,
                              'gradients_default_optimizer_global_norm': 2.427621841430664,
                              'mean_kl_loss': 0.018026262521743774,
                              'module_train_batch_size_mean': 128.0,
                              'num_module_steps_trained': 11264,
                              'num_module_steps_trained_lifetime': 22528,
                              'num_trainable_parameters': 142093.0,
                              'policy_loss': -0.021811559796333313,
                              'total_loss': 0.36956357955932617,
                              'vf_explained_var': 0.002324342727661133,
                              'vf_loss': 0.38778623938560486,
                              'vf_loss_unclipped': 0.38778623938560486,
                              'weights_seq_no': 2.0},
                'target': { 'curr_entropy_coeff': 2e-06,
                            'curr_kl_coeff': 0.20000000298023224,
                            'default_optimizer_learning_rate': 0.0003,
                            'diff_num_grad_updates_vs_sampler_policy': 1.0,
                            'entropy': 8.52366828918457,
                            'gradients_default_optimizer_global_norm': 1.2126866579055786,
                            'mean_kl_loss': 0.015317443758249283,
                            'module_train_batch_size_mean': 128.0,
                            'num_module_steps_trained': 11264,
                            'num_module_steps_trained_lifetime': 22528,
                            'num_trainable_parameters': 142093.0,
                            'policy_loss': 0.05511286109685898,
                            'total_loss': 0.05826028436422348,
                            'vf_explained_var': 0.020294666290283203,
                            'vf_loss': 0.00010097067570313811,
                            'vf_loss_unclipped': 0.00010097067570313811,
                            'weights_seq_no': 2.0}},
  'node_ip': '127.0.0.1',
  'num_env_steps_sampled_lifetime': 5600,
  'num_env_steps_sampled_lifetime_throughput': 674.0713194016384,
  'num_training_step_calls_per_iteration': 1,
  'perf': {'cpu_util_percent': 75.64999999999999, 'ram_util_percent': 66.55},
  'pid': 79425,
  'time_since_restore': 8.875931739807129,
  'time_this_iter_s': 4.104266881942749,
  'time_total_s': 8.875931739807129,
  'timers': { 'env_runner_sampling_timer': 1.966303799574962,
              'learner_update_timer': 2.7610691497597144,
              'restore_env_runners': 3.5773840500041844e-05,
              'synch_env_connectors': 0.012859113048762083,
              'synch_weights': 0.011026894327951595,
              'training_iteration': 4.74120441081759,
              'training_step': 4.740820499332622},
  'timestamp': 1745348665,
  'training_iteration': 2,
  'trial_id': 'default'}
2025-04-22 15:04:25,759 [__main__] [ERROR] NaN detected in losses for both agents at iteration 2. Stopping training.
2025-04-22 15:04:25,759 [__main__] [INFO] 
--- Training Loop Finished ---
2025-04-22 15:04:25,759 [__main__] [INFO] Total Training Time: 8.98 seconds
2025-04-22 15:04:25,759 [__main__] [INFO] Last completed iteration: 2
2025-04-22 15:04:25,759 [__main__] [INFO] Attempting to save final checkpoint...
2025-04-22 15:04:25,815 [__main__] [INFO] Final checkpoint saved successfully at: /var/folders/92/t36tz1yx71q_33zknvm23p_00000gn/T/tmppgtyce_d
2025-04-22 15:04:25,815 [__main__] [INFO] Running final evaluation...
2025-04-22 15:04:25,815 [__main__] [INFO] 
--- Running Evaluation & Recording Video ---
2025-04-22 15:04:25,815 [__main__] [ERROR] Final evaluation failed: `uses_new_env_runners` has been deprecated. Use `AlgorithmConfig.enable_env_runner_and_connector_v2` instead.
Traceback (most recent call last):
  File "/Users/leo/RL-Spacecraft-Docking/src/train_marl.py", line 672, in <module>
    run_evaluation_video(algo, satellite_pettingzoo_creator, num_episodes=EVAL_EPISODES, max_steps=EVAL_MAX_STEPS)
  File "/Users/leo/RL-Spacecraft-Docking/src/train_marl.py", line 82, in run_evaluation_video
    use_new_api_inference = algo.config.uses_new_env_runners() and algo.config.uses_new_api_stack()
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/leo/RL-Spacecraft-Docking/.venv/lib/python3.11/site-packages/ray/rllib/utils/deprecation.py", line 121, in _ctor
    deprecation_warning(
  File "/Users/leo/RL-Spacecraft-Docking/.venv/lib/python3.11/site-packages/ray/rllib/utils/deprecation.py", line 48, in deprecation_warning
    raise ValueError(msg)
ValueError: `uses_new_env_runners` has been deprecated. Use `AlgorithmConfig.enable_env_runner_and_connector_v2` instead.
2025-04-22 15:04:25,819 [__main__] [INFO] Stopping RLlib Algorithm...
2025-04-22 15:04:25,927 [src.satellite_marl_env] [INFO] SatelliteMARLEnv closed.
2025-04-22 15:04:25,945 [src.satellite_marl_env] [INFO] SatelliteMARLEnv closed.
2025-04-22 15:04:25,965 [__main__] [INFO] Algorithm stopped.
2025-04-22 15:04:25,965 [__main__] [INFO] Shutting down Ray...
2025-04-22 15:04:27,619 [__main__] [INFO] Ray shut down.
2025-04-22 15:04:27,620 [__main__] [INFO] Script finished.
